"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[2389],{6973:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"module4-vla/manipulation-grasping","title":"Manipulation and Grasping","description":"Overview","source":"@site/docs/module4-vla/manipulation-grasping.md","sourceDirName":"module4-vla","slug":"/module4-vla/manipulation-grasping","permalink":"/docusaurus-robotics-book/pa/module4-vla/manipulation-grasping","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion","permalink":"/docusaurus-robotics-book/pa/module4-vla/bipedal-locomotion"},"next":{"title":"Human-Robot Interaction Design","permalink":"/docusaurus-robotics-book/pa/module4-vla/hri-design"}}');var s=i(4848),a=i(8453);const o={},t="Manipulation and Grasping",l={},p=[{value:"Overview",id:"overview",level:2},{value:"Grasping Fundamentals",id:"grasping-fundamentals",level:2},{value:"Types of Grasps",id:"types-of-grasps",level:3},{value:"1. Power Grasps",id:"1-power-grasps",level:4},{value:"2. Precision Grasps",id:"2-precision-grasps",level:4},{value:"Grasp Quality Metrics",id:"grasp-quality-metrics",level:3},{value:"Grasp Planning",id:"grasp-planning",level:2},{value:"Grasp Pose Generation",id:"grasp-pose-generation",level:3},{value:"Grasp Synthesis with Deep Learning",id:"grasp-synthesis-with-deep-learning",level:3},{value:"Force Control",id:"force-control",level:2},{value:"Impedance Control",id:"impedance-control",level:3},{value:"Hybrid Force-Position Control",id:"hybrid-force-position-control",level:3},{value:"Dexterous Manipulation",id:"dexterous-manipulation",level:2},{value:"Fingertip Control",id:"fingertip-control",level:3},{value:"In-Hand Manipulation",id:"in-hand-manipulation",level:3},{value:"Vision-Based Grasping",id:"vision-based-grasping",level:2},{value:"RGB-D Grasp Detection",id:"rgb-d-grasp-detection",level:3},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Grasp Planning",id:"exercise-1-grasp-planning",level:3},{value:"Exercise 2: Force Control",id:"exercise-2-force-control",level:3},{value:"Exercise 3: Vision-Based Grasping",id:"exercise-3-vision-based-grasping",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"manipulation-and-grasping",children:"Manipulation and Grasping"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"Manipulation and grasping enable humanoid robots to interact with objects in their environment. This module covers the principles of robot grasping, force control, and dexterous manipulation."}),"\n",(0,s.jsx)(e.h2,{id:"grasping-fundamentals",children:"Grasping Fundamentals"}),"\n",(0,s.jsx)(e.h3,{id:"types-of-grasps",children:"Types of Grasps"}),"\n",(0,s.jsx)(e.h4,{id:"1-power-grasps",children:"1. Power Grasps"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cylindrical"}),": Wrapping fingers around cylinder (e.g., holding a bottle)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spherical"}),": Cupping hand around sphere (e.g., holding a ball)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hook"}),": Fingers hooked (e.g., carrying a bag)"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"2-precision-grasps",children:"2. Precision Grasps"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Pinch"}),": Thumb and fingertip opposition (e.g., picking a coin)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tripod"}),": Thumb and two fingers (e.g., holding a pen)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Lateral"}),": Thumb against side of finger (e.g., holding a key)"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"grasp-quality-metrics",children:"Grasp Quality Metrics"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'def grasp_quality_epsilon(contact_points, contact_normals, friction_coeff):\n    """\n    Compute grasp quality (epsilon metric)\n    Measures minimum force needed to resist arbitrary wrench\n    """\n\n    # Build grasp matrix G\n    G = compute_grasp_matrix(contact_points, contact_normals)\n\n    # Compute minimum singular value\n    U, s, Vh = np.linalg.svd(G)\n\n    # Epsilon metric is minimum singular value\n    epsilon = np.min(s)\n\n    return epsilon\n\ndef compute_grasp_matrix(contact_points, normals):\n    """Compute grasp matrix from contacts"""\n\n    G = []\n    for point, normal in zip(contact_points, normals):\n        # Force wrench from this contact\n        # [force_x, force_y, force_z, torque_x, torque_y, torque_z]\n\n        # Simplified for point contacts with friction\n        wrench = np.zeros((6, 3))\n        wrench[:3, :] = np.eye(3)  # Force\n        wrench[3:, :] = skew_symmetric(point)  # Torque = r x F\n\n        G.append(wrench)\n\n    return np.hstack(G)\n\ndef skew_symmetric(v):\n    """Convert vector to skew-symmetric matrix"""\n    return np.array([\n        [0, -v[2], v[1]],\n        [v[2], 0, -v[0]],\n        [-v[1], v[0], 0]\n    ])\n'})}),"\n",(0,s.jsx)(e.h2,{id:"grasp-planning",children:"Grasp Planning"}),"\n",(0,s.jsx)(e.h3,{id:"grasp-pose-generation",children:"Grasp Pose Generation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import trimesh\nimport numpy as np\n\nclass GraspPlanner:\n    def __init__(self, mesh_path):\n        self.mesh = trimesh.load(mesh_path)\n\n    def sample_grasp_poses(self, num_samples=100):\n        """Sample candidate grasp poses on object surface"""\n\n        grasps = []\n\n        for _ in range(num_samples):\n            # Sample point on surface\n            point, face_idx = self.mesh.sample(1, return_index=True)\n            point = point[0]\n\n            # Surface normal at point\n            normal = self.mesh.face_normals[face_idx[0]]\n\n            # Approach direction (opposite to normal)\n            approach = -normal\n\n            # Create grasp pose\n            grasp_pose = self.create_grasp_pose(point, approach)\n            grasps.append(grasp_pose)\n\n        return grasps\n\n    def create_grasp_pose(self, position, approach):\n        """Create 4x4 grasp transformation matrix"""\n\n        # Gripper frame: z-axis along approach direction\n        z_axis = approach / np.linalg.norm(approach)\n\n        # Choose arbitrary x-axis perpendicular to z\n        if abs(z_axis[2]) < 0.9:\n            x_axis = np.cross(z_axis, [0, 0, 1])\n        else:\n            x_axis = np.cross(z_axis, [1, 0, 0])\n\n        x_axis = x_axis / np.linalg.norm(x_axis)\n\n        # y-axis completes right-hand frame\n        y_axis = np.cross(z_axis, x_axis)\n\n        # Build transformation matrix\n        T = np.eye(4)\n        T[:3, 0] = x_axis\n        T[:3, 1] = y_axis\n        T[:3, 2] = z_axis\n        T[:3, 3] = position\n\n        return T\n\n    def evaluate_grasps(self, grasps):\n        """Rank grasps by quality metrics"""\n\n        scored_grasps = []\n\n        for grasp in grasps:\n            # Simulate contact points\n            contacts = self.simulate_contacts(grasp)\n\n            if len(contacts) < 2:\n                continue  # Need at least two contacts\n\n            # Compute quality\n            quality = self.compute_grasp_quality(contacts)\n\n            # Check other criteria\n            if self.is_collision_free(grasp) and self.is_reachable(grasp):\n                scored_grasps.append((grasp, quality))\n\n        # Sort by quality\n        scored_grasps.sort(key=lambda x: x[1], reverse=True)\n\n        return scored_grasps\n'})}),"\n",(0,s.jsx)(e.h3,{id:"grasp-synthesis-with-deep-learning",children:"Grasp Synthesis with Deep Learning"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\n\nclass GraspQualityNet(nn.Module):\n    """Neural network to predict grasp quality from depth image"""\n\n    def __init__(self):\n        super().__init__()\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n            nn.ReLU()\n        )\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128 * 7 * 7, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1),\n            nn.Sigmoid()  # Quality score between 0 and 1\n        )\n\n    def forward(self, depth_image):\n        x = self.conv_layers(depth_image)\n        x = x.view(x.size(0), -1)\n        quality = self.fc_layers(x)\n        return quality\n\n# Usage\nmodel = GraspQualityNet()\ndepth_image = capture_depth_image()\ngrasp_quality = model(depth_image)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"force-control",children:"Force Control"}),"\n",(0,s.jsx)(e.h3,{id:"impedance-control",children:"Impedance Control"}),"\n",(0,s.jsx)(e.p,{children:"Control robot compliance (stiffness and damping):"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class ImpedanceController:\n    def __init__(self, K_stiffness, D_damping):\n        self.K = np.diag(K_stiffness)  # Stiffness matrix\n        self.D = np.diag(D_damping)     # Damping matrix\n\n    def compute_force(self, x_desired, x_current, v_current):\n        """Compute desired force for impedance control"""\n\n        # Position error\n        error = x_desired - x_current\n\n        # Impedance control law: F = K * error - D * velocity\n        F = self.K @ error - self.D @ v_current\n\n        return F\n\n# Example: Soft contact\n# High stiffness, low damping = rigid\nstiff_controller = ImpedanceController(\n    K_stiffness=[1000, 1000, 1000],\n    D_damping=[50, 50, 50]\n)\n\n# Low stiffness, high damping = compliant\ncompliant_controller = ImpedanceController(\n    K_stiffness=[100, 100, 100],\n    D_damping=[200, 200, 200]\n)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"hybrid-force-position-control",children:"Hybrid Force-Position Control"}),"\n",(0,s.jsx)(e.p,{children:"Control position in some directions, force in others:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class HybridController:\n    def __init__(self, selection_matrix):\n        self.S = selection_matrix  # Diagonal: 1 = force control, 0 = position control\n\n    def compute_control(self, x_desired, F_desired, x_current, F_current):\n        """Hybrid force-position control"""\n\n        # Position error\n        x_error = x_desired - x_current\n\n        # Force error\n        F_error = F_desired - F_current\n\n        # Position control in selected directions\n        u_position = self.Kp * (np.eye(3) - self.S) @ x_error\n\n        # Force control in selected directions\n        u_force = self.Kf * self.S @ F_error\n\n        # Combined control\n        u = u_position + u_force\n\n        return u\n\n# Example: Pushing down (control z-force, free x-y position)\ncontroller = HybridController(\n    selection_matrix=np.diag([0, 0, 1])  # Force control in z only\n)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"dexterous-manipulation",children:"Dexterous Manipulation"}),"\n",(0,s.jsx)(e.h3,{id:"fingertip-control",children:"Fingertip Control"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class DexterousHand:\n    def __init__(self, num_fingers=5):\n        self.num_fingers = num_fingers\n\n    def fingertip_ik(self, target_positions):\n        """Inverse kinematics for fingertips"""\n\n        joint_angles = []\n\n        for finger_idx, target in enumerate(target_positions):\n            # IK for each finger\n            angles = self.finger_ik(finger_idx, target)\n            joint_angles.append(angles)\n\n        return joint_angles\n\n    def compute_grasp_matrix(self, fingertip_positions):\n        """Compute grasp matrix for current fingertip configuration"""\n\n        # Similar to earlier grasp matrix computation\n        # but for multi-finger hand\n\n        return grasp_matrix\n'})}),"\n",(0,s.jsx)(e.h3,{id:"in-hand-manipulation",children:"In-Hand Manipulation"}),"\n",(0,s.jsx)(e.p,{children:"Reorienting objects within the hand:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def plan_in_hand_manipulation(object_pose_current, object_pose_desired):\n    \"\"\"\n    Plan finger motions to reorient object\n    \"\"\"\n\n    # Compute relative rotation\n    rotation_needed = compute_relative_rotation(\n        object_pose_current,\n        object_pose_desired\n    )\n\n    # Plan sequence of finger gaits\n    # Similar to walking: some fingers maintain contact, others reposition\n\n    manipulation_plan = []\n\n    # Phase 1: Fingers 1,2,3 hold, fingers 4,5 reposition\n    manipulation_plan.append({\n        'holding_fingers': [0, 1, 2],\n        'moving_fingers': [3, 4],\n        'object_rotation': rotation_needed * 0.5\n    })\n\n    # Phase 2: Fingers 4,5 hold, fingers 1,2,3 reposition\n    manipulation_plan.append({\n        'holding_fingers': [3, 4],\n        'moving_fingers': [0, 1, 2],\n        'object_rotation': rotation_needed * 0.5\n    })\n\n    return manipulation_plan\n"})}),"\n",(0,s.jsx)(e.h2,{id:"vision-based-grasping",children:"Vision-Based Grasping"}),"\n",(0,s.jsx)(e.h3,{id:"rgb-d-grasp-detection",children:"RGB-D Grasp Detection"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class VisionGraspPlanner:\n    def __init__(self, camera):\n        self.camera = camera\n\n    def detect_grasp_from_rgbd(self):\n        """Detect graspable objects and plan grasps"""\n\n        # Capture RGB-D image\n        rgb = self.camera.get_rgb()\n        depth = self.camera.get_depth()\n\n        # Segment objects\n        masks = self.segment_objects(rgb)\n\n        grasps = []\n        for mask in masks:\n            # Extract object point cloud\n            points = self.depth_to_pointcloud(depth, mask)\n\n            # Fit primitive shape (cylinder, box, sphere)\n            shape, params = self.fit_shape(points)\n\n            # Generate grasp based on shape\n            grasp = self.shape_based_grasp(shape, params)\n            grasps.append(grasp)\n\n        return grasps\n\n    def segment_objects(self, rgb_image):\n        """Segment objects using vision model"""\n        # Use Segment Anything, Detectron2, or similar\n        pass\n\n    def depth_to_pointcloud(self, depth, mask):\n        """Convert depth image to 3D point cloud"""\n\n        points = []\n        h, w = depth.shape\n\n        for v in range(h):\n            for u in range(w):\n                if mask[v, u]:\n                    z = depth[v, u]\n                    x = (u - self.camera.cx) * z / self.camera.fx\n                    y = (v - self.camera.cy) * z / self.camera.fy\n                    points.append([x, y, z])\n\n        return np.array(points)\n'})}),"\n",(0,s.jsx)(e.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,s.jsx)(e.h3,{id:"exercise-1-grasp-planning",children:"Exercise 1: Grasp Planning"}),"\n",(0,s.jsx)(e.p,{children:"Implement basic grasp planning:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Load object mesh\n# 2. Sample grasp candidates\n# 3. Evaluate grasp quality\n# 4. Visualize top 10 grasps\n"})}),"\n",(0,s.jsx)(e.h3,{id:"exercise-2-force-control",children:"Exercise 2: Force Control"}),"\n",(0,s.jsx)(e.p,{children:"Implement impedance control for compliant grasping:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Set up robot with gripper\n# 2. Implement impedance controller\n# 3. Grasp deformable object (sponge)\n# 4. Measure contact forces\n"})}),"\n",(0,s.jsx)(e.h3,{id:"exercise-3-vision-based-grasping",children:"Exercise 3: Vision-Based Grasping"}),"\n",(0,s.jsx)(e.p,{children:"Implement full vision-to-grasp pipeline:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Capture RGB-D image\n# 2. Detect objects\n# 3. Plan grasp for each object\n# 4. Execute best grasp\n"})}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Grasp quality depends on contact geometry and friction"}),"\n",(0,s.jsx)(e.li,{children:"Multiple grasp planning approaches: analytical, sampling, learning"}),"\n",(0,s.jsx)(e.li,{children:"Force control enables safe interaction with objects"}),"\n",(0,s.jsx)(e.li,{children:"Dexterous manipulation requires coordinating multiple fingers"}),"\n",(0,s.jsx)(e.li,{children:"Vision is essential for detecting and grasping unknown objects"}),"\n",(0,s.jsx)(e.li,{children:"Simulation is crucial for testing grasp strategies safely"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://manipulation.csail.mit.edu/pick.html",children:"Grasp Planning Tutorial"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://graspit-simulator.github.io/",children:"GraspIt! Simulator"})}),"\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.a,{href:"https://berkeleyautomation.github.io/dex-net/",children:"Dex-Net: Deep Learning for Grasp Planning"})}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(e.p,{children:["Continue to ",(0,s.jsx)(e.a,{href:"/docusaurus-robotics-book/pa/module4-vla/hri-design",children:"Human-Robot Interaction Design"})," to learn about natural interaction patterns."]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var r=i(6540);const s={},a=r.createContext(s);function o(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);