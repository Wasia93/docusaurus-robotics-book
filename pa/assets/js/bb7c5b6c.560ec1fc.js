"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[2477],{3297:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>t,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module3-isaac/reinforcement-learning","title":"Reinforcement Learning for Robotics","description":"Training humanoid robots with RL in Isaac Sim","source":"@site/docs/module3-isaac/reinforcement-learning.md","sourceDirName":"module3-isaac","slug":"/module3-isaac/reinforcement-learning","permalink":"/docusaurus-robotics-book/pa/module3-isaac/reinforcement-learning","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Reinforcement Learning for Robotics","description":"Training humanoid robots with RL in Isaac Sim","keywords":["reinforcement learning","RL","Isaac Sim","robot learning","sim-to-real"]},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Bipedal Humanoids","permalink":"/docusaurus-robotics-book/pa/module3-isaac/nav2-bipedal"},"next":{"title":"Voice-to-Action Pipeline","permalink":"/docusaurus-robotics-book/pa/module4-vla/voice-to-action"}}');var s=i(4848),o=i(8453);const l={sidebar_position:5,title:"Reinforcement Learning for Robotics",description:"Training humanoid robots with RL in Isaac Sim",keywords:["reinforcement learning","RL","Isaac Sim","robot learning","sim-to-real"]},a="Reinforcement Learning for Humanoid Robots",t={},d=[{value:"Introduction to RL for Robotics",id:"introduction-to-rl-for-robotics",level:2},{value:"Why RL for Humanoids?",id:"why-rl-for-humanoids",level:3},{value:"RL Fundamentals",id:"rl-fundamentals",level:2},{value:"Markov Decision Process (MDP)",id:"markov-decision-process-mdp",level:3},{value:"RL Algorithm Categories",id:"rl-algorithm-categories",level:3},{value:"RL in Isaac Sim",id:"rl-in-isaac-sim",level:2},{value:"Isaac Gym",id:"isaac-gym",level:3},{value:"Example Task: Humanoid Walk",id:"example-task-humanoid-walk",level:2},{value:"State Space",id:"state-space",level:3},{value:"Action Space",id:"action-space",level:3},{value:"Reward Function",id:"reward-function",level:3},{value:"Training with PPO",id:"training-with-ppo",level:2},{value:"PPO Algorithm",id:"ppo-algorithm",level:3},{value:"Isaac Gym PPO Example",id:"isaac-gym-ppo-example",level:3},{value:"PPO Training Script",id:"ppo-training-script",level:3},{value:"Parallelization",id:"parallelization",level:3},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Randomize Physics",id:"randomize-physics",level:3},{value:"Randomize Visuals",id:"randomize-visuals",level:3},{value:"Randomize Observations",id:"randomize-observations",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Workflow",id:"workflow",level:3},{value:"Challenges",id:"challenges",level:3},{value:"Pre-Trained Humanoid Skills",id:"pre-trained-humanoid-skills",level:2},{value:"Advanced: Hierarchical RL",id:"advanced-hierarchical-rl",level:2},{value:"Monitoring Training",id:"monitoring-training",level:2},{value:"TensorBoard",id:"tensorboard",level:3},{value:"Evaluation",id:"evaluation",level:3},{value:"Sample Efficiency",id:"sample-efficiency",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Lab Exercise",id:"lab-exercise",level:2}];function c(n){const e={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"reinforcement-learning-for-humanoid-robots",children:"Reinforcement Learning for Humanoid Robots"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction-to-rl-for-robotics",children:"Introduction to RL for Robotics"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Reinforcement Learning (RL)"})," enables robots to learn behaviors through trial and error:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Exploration"}),": Try different actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feedback"}),": Receive rewards/penalties"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Learning"}),": Improve policy to maximize cumulative reward"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"why-rl-for-humanoids",children:"Why RL for Humanoids?"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Traditional approach"})," (hand-coded):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Program every behavior explicitly"}),"\n",(0,s.jsx)(e.li,{children:"Fragile (breaks in new situations)"}),"\n",(0,s.jsx)(e.li,{children:"Time-consuming to develop"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"RL approach"})," (learned):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Robot learns from experience"}),"\n",(0,s.jsx)(e.li,{children:"Generalizes to new situations"}),"\n",(0,s.jsx)(e.li,{children:"Discovers optimal behaviors automatically"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Use cases"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Locomotion"}),": Learn to walk, run, navigate terrain"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulation"}),": Grasp diverse objects"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Whole-body control"}),": Coordinate arms, legs, torso"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"rl-fundamentals",children:"RL Fundamentals"}),"\n",(0,s.jsx)(e.h3,{id:"markov-decision-process-mdp",children:"Markov Decision Process (MDP)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Components"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State (s)"}),": Robot configuration, sensor readings"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action (a)"}),": Motor commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reward (r)"}),": Scalar feedback (positive = good, negative = bad)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Policy (\u03c0)"}),": Mapping from state to action"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Value function (V)"}),": Expected cumulative reward"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Goal"}),": Learn policy \u03c0* that maximizes expected cumulative reward."]}),"\n",(0,s.jsx)(e.h3,{id:"rl-algorithm-categories",children:"RL Algorithm Categories"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Algorithm"}),(0,s.jsx)(e.th,{children:"Type"}),(0,s.jsx)(e.th,{children:"On/Off-Policy"}),(0,s.jsx)(e.th,{children:"Use Case"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"PPO"})}),(0,s.jsx)(e.td,{children:"Policy Gradient"}),(0,s.jsx)(e.td,{children:"On-policy"}),(0,s.jsx)(e.td,{children:"General-purpose, stable"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"SAC"})}),(0,s.jsx)(e.td,{children:"Actor-Critic"}),(0,s.jsx)(e.td,{children:"Off-policy"}),(0,s.jsx)(e.td,{children:"Continuous control"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"TD3"})}),(0,s.jsx)(e.td,{children:"Actor-Critic"}),(0,s.jsx)(e.td,{children:"Off-policy"}),(0,s.jsx)(e.td,{children:"High-dimensional"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"DQN"})}),(0,s.jsx)(e.td,{children:"Value-based"}),(0,s.jsx)(e.td,{children:"Off-policy"}),(0,s.jsx)(e.td,{children:"Discrete actions"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"TRPO"})}),(0,s.jsx)(e.td,{children:"Policy Gradient"}),(0,s.jsx)(e.td,{children:"On-policy"}),(0,s.jsx)(e.td,{children:"Safe, monotonic improvement"})]})]})]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Most popular for robotics"}),": ",(0,s.jsx)(e.strong,{children:"PPO"})," (Proximal Policy Optimization) - stable and sample-efficient."]}),"\n",(0,s.jsx)(e.h2,{id:"rl-in-isaac-sim",children:"RL in Isaac Sim"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Sim"})," is ideal for RL:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Fast simulation"}),": Train 1000x faster than real-time"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallel environments"}),": Run 100s of robots simultaneously"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Domain randomization"}),": Vary physics, visuals for robustness"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Synthetic data"}),": No need for real-world data collection"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"isaac-gym",children:"Isaac Gym"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"}),": NVIDIA's RL platform (part of Isaac Sim)."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Features"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"GPU-accelerated physics (PhysX)"}),"\n",(0,s.jsx)(e.li,{children:"Tensor-based API (direct PyTorch integration)"}),"\n",(0,s.jsx)(e.li,{children:"Massive parallelization (4096+ environments)"}),"\n",(0,s.jsx)(e.li,{children:"Pre-built humanoid tasks"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"example-task-humanoid-walk",children:"Example Task: Humanoid Walk"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Goal"}),": Train humanoid to walk forward."]}),"\n",(0,s.jsx)(e.h3,{id:"state-space",children:"State Space"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Observation"})," (what robot senses):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"observation = [\n    # Joint positions (30 values for 30 DOF)\n    joint_positions,  # rad\n\n    # Joint velocities (30 values)\n    joint_velocities,  # rad/s\n\n    # Torso orientation (4 values: quaternion)\n    torso_orientation,\n\n    # Torso linear velocity (3 values)\n    torso_velocity,\n\n    # Torso angular velocity (3 values)\n    torso_angular_velocity,\n\n    # Contact forces (4 values: both feet)\n    left_foot_contact,\n    right_foot_contact,\n]\n# Total: ~70 observations\n"})}),"\n",(0,s.jsx)(e.h3,{id:"action-space",children:"Action Space"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Action"})," (what robot controls):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"action = [\n    # Target joint positions (PD control)\n    # OR target joint torques (direct torque control)\n\n    # For 30 DOF humanoid: 30 values\n]\n\n# Actions are typically in range [-1, 1] (normalized)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"reward-function",children:"Reward Function"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Reward"})," guides learning:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_reward(obs, action):\n    # Reward forward velocity\n    reward_forward = obs['torso_velocity'][0]  # x-axis velocity\n\n    # Penalty for falling\n    reward_upright = 1.0 if obs['torso_height'] > 0.8 else -10.0\n\n    # Penalty for high energy (smooth movements)\n    reward_energy = -0.01 * np.sum(action ** 2)\n\n    # Penalty for orientation deviation\n    reward_orientation = -np.abs(obs['torso_roll']) - np.abs(obs['torso_pitch'])\n\n    # Total reward\n    reward = (\n        2.0 * reward_forward +\n        1.0 * reward_upright +\n        0.5 * reward_energy +\n        1.0 * reward_orientation\n    )\n\n    return reward\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key principles"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reward progress"}),": Forward velocity"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Penalize failure"}),": Falling, tipping over"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Encourage efficiency"}),": Low energy consumption"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Maintain balance"}),": Upright orientation"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"training-with-ppo",children:"Training with PPO"}),"\n",(0,s.jsx)(e.h3,{id:"ppo-algorithm",children:"PPO Algorithm"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"PPO"})," (Proximal Policy Optimization):"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Collect trajectories using current policy"}),"\n",(0,s.jsx)(e.li,{children:"Compute advantage estimates (how much better than average)"}),"\n",(0,s.jsx)(e.li,{children:"Update policy with clipped objective (prevent large updates)"}),"\n",(0,s.jsx)(e.li,{children:"Repeat until convergence"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"isaac-gym-ppo-example",children:"Isaac Gym PPO Example"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from omni.isaac.gym.vec_env import VecEnvBase\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\n\nclass HumanoidWalkEnv(VecEnvBase):\n    def __init__(self, cfg, sim_device, graphics_device_id, headless):\n        self.cfg = cfg\n        self.num_envs = cfg["env"]["numEnvs"]\n        self.num_obs = 70\n        self.num_actions = 30\n\n        super().__init__(cfg, sim_device, graphics_device_id, headless)\n\n        # Buffers\n        self.obs_buf = torch.zeros((self.num_envs, self.num_obs), device=self.device)\n        self.reward_buf = torch.zeros(self.num_envs, device=self.device)\n        self.reset_buf = torch.zeros(self.num_envs, dtype=torch.long, device=self.device)\n\n    def step(self, actions):\n        # Apply actions to robots\n        self.apply_actions(actions)\n\n        # Step simulation\n        self.sim.step()\n\n        # Compute observations\n        self.compute_observations()\n\n        # Compute rewards\n        self.compute_rewards()\n\n        # Check for resets (fell, timeout)\n        self.check_termination()\n\n        return self.obs_buf, self.reward_buf, self.reset_buf, {}\n\n    def compute_observations(self):\n        # Get joint states from simulation\n        joint_pos = self.get_joint_positions()\n        joint_vel = self.get_joint_velocities()\n        torso_state = self.get_torso_state()\n\n        # Concatenate into observation\n        self.obs_buf = torch.cat([joint_pos, joint_vel, torso_state], dim=-1)\n\n    def compute_rewards(self):\n        # Get relevant states\n        torso_vel = self.obs_buf[:, 60:63]  # Indices for velocity\n        torso_height = self.obs_buf[:, 63]\n\n        # Reward forward velocity\n        reward_forward = torso_vel[:, 0]  # x-axis\n\n        # Penalty for falling\n        reward_upright = torch.where(\n            torso_height > 0.8,\n            torch.ones_like(torso_height),\n            -10.0 * torch.ones_like(torso_height)\n        )\n\n        # Total reward\n        self.reward_buf = 2.0 * reward_forward + reward_upright\n\n    def check_termination(self):\n        # Reset if fallen or timeout\n        torso_height = self.obs_buf[:, 63]\n        fallen = torso_height < 0.5\n\n        self.reset_buf = torch.where(\n            fallen,\n            torch.ones_like(self.reset_buf),\n            self.reset_buf\n        )\n\n    def reset(self):\n        # Reset robots to initial state\n        self.set_initial_state()\n        self.compute_observations()\n        return self.obs_buf\n'})}),"\n",(0,s.jsx)(e.h3,{id:"ppo-training-script",children:"PPO Training Script"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import torch\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\n# Create environment\nenv = HumanoidWalkEnv(cfg, sim_device='cuda:0', graphics_device_id=0, headless=False)\n\n# Create PPO agent\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    learning_rate=3e-4,\n    n_steps=2048,\n    batch_size=64,\n    n_epochs=10,\n    gamma=0.99,\n    gae_lambda=0.95,\n    clip_range=0.2,\n    verbose=1,\n    device='cuda'\n)\n\n# Train\nmodel.learn(total_timesteps=10_000_000)\n\n# Save model\nmodel.save(\"humanoid_walk_ppo\")\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Training time"}),": 1-2 hours on RTX 4080 (10M timesteps)."]}),"\n",(0,s.jsx)(e.h3,{id:"parallelization",children:"Parallelization"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Key to fast training"}),": Run many environments simultaneously."]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Single environment: 10 FPS, 10M steps = 11 days\n# 1024 parallel environments: 10 FPS \xd7 1024 = 10,240 steps/sec\n# 10M steps / 10,240 = 16 minutes!\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"})," can run 4096+ environments on a single GPU."]}),"\n",(0,s.jsx)(e.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Problem"}),": Policies trained in simulation don't work on real robot (sim-to-real gap)."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),": ",(0,s.jsx)(e.strong,{children:"Domain Randomization"})," - vary simulation parameters during training."]}),"\n",(0,s.jsx)(e.h3,{id:"randomize-physics",children:"Randomize Physics"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Mass randomization\nfor link in humanoid.links:\n    link.mass *= np.random.uniform(0.8, 1.2)\n\n# Friction randomization\nfor contact in humanoid.contacts:\n    contact.friction = np.random.uniform(0.5, 1.5)\n\n# Joint damping randomization\nfor joint in humanoid.joints:\n    joint.damping *= np.random.uniform(0.8, 1.2)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"randomize-visuals",children:"Randomize Visuals"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Lighting randomization\nlight.intensity = np.random.uniform(500, 2000)\nlight.color = np.random.uniform([0.8, 0.8, 0.8], [1.0, 1.0, 1.0])\n\n# Texture randomization\nrobot.material.base_color = np.random.uniform([0, 0, 0], [1, 1, 1])\nrobot.material.roughness = np.random.uniform(0.0, 1.0)\n"})}),"\n",(0,s.jsx)(e.h3,{id:"randomize-observations",children:"Randomize Observations"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Add noise to observations\nobs_noisy = obs + np.random.normal(0, 0.01, size=obs.shape)\n\n# Simulate sensor lag\nobs_delayed = obs_history[t - lag]\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Result"}),": Robust policy that generalizes to real robot."]}),"\n",(0,s.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(e.h3,{id:"workflow",children:"Workflow"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Train in simulation"})," (Isaac Sim):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use domain randomization"}),"\n",(0,s.jsx)(e.li,{children:"Train for millions of timesteps"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate in varied simulated conditions"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Export policy"})," (neural network weights):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'model.save("humanoid_walk_policy.pth")\n'})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Deploy to real robot"})," (Jetson Orin Nano):"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'model = torch.load("humanoid_walk_policy.pth")\nmodel.eval()\n\n# Inference loop\nobs = get_robot_state()\naction = model(obs)\nexecute_action(action)\n'})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Fine-tune"})," (optional):"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Collect small amount of real-world data"}),"\n",(0,s.jsx)(e.li,{children:"Fine-tune policy on real robot"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"challenges",children:"Challenges"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Sim-to-real gap sources"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Physics approximation (contacts, friction)"}),"\n",(0,s.jsx)(e.li,{children:"Sensor noise (real sensors noisier than simulated)"}),"\n",(0,s.jsx)(e.li,{children:"Actuator dynamics (delays, backlash)"}),"\n",(0,s.jsx)(e.li,{children:"Unmodeled effects (cable drag, wear)"}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Mitigations"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Domain randomization (covers wide range of conditions)"}),"\n",(0,s.jsx)(e.li,{children:"System identification (match simulation to real robot)"}),"\n",(0,s.jsx)(e.li,{children:"Residual learning (learn correction on top of sim policy)"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"pre-trained-humanoid-skills",children:"Pre-Trained Humanoid Skills"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"})," includes pre-trained policies:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Walking"}),": Forward, backward, turning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Balancing"}),": Maintain upright under pushes"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Reaching"}),": Arm control to target positions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Grasping"}),": Hand manipulation"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Use pre-trained policies"})," as initialization for new tasks (transfer learning)."]}),"\n",(0,s.jsx)(e.h2,{id:"advanced-hierarchical-rl",children:"Advanced: Hierarchical RL"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"For complex tasks"}),": Combine multiple skills."]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:'High-level policy: "Navigate to kitchen"\n    \u2193\nMid-level skills: [Walk forward, Turn left, Open door]\n    \u2193\nLow-level control: Joint torques\n'})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Benefits"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Faster learning (reuse low-level skills)"}),"\n",(0,s.jsx)(e.li,{children:"Better generalization"}),"\n",(0,s.jsx)(e.li,{children:"Interpretable behavior"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"monitoring-training",children:"Monitoring Training"}),"\n",(0,s.jsx)(e.h3,{id:"tensorboard",children:"TensorBoard"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n\nfor episode in range(num_episodes):\n    # Training...\n    writer.add_scalar('Reward/Episode', episode_reward, episode)\n    writer.add_scalar('Length/Episode', episode_length, episode)\n\n# View in browser\n# tensorboard --logdir=runs\n"})}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Metrics to track"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Episode reward (should increase)"}),"\n",(0,s.jsx)(e.li,{children:"Episode length (should increase)"}),"\n",(0,s.jsx)(e.li,{children:"Policy loss (should decrease)"}),"\n",(0,s.jsx)(e.li,{children:"Value loss (should decrease)"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"evaluation",children:"Evaluation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Evaluate trained policy\nobs = env.reset()\ntotal_reward = 0\n\nfor step in range(1000):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, _ = env.step(action)\n    total_reward += reward\n\n    if done:\n        break\n\nprint(f"Evaluation reward: {total_reward}")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"sample-efficiency",children:"Sample Efficiency"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Challenge"}),": RL requires millions of samples."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Strategies"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Off-policy algorithms"})," (SAC, TD3): Reuse old data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Model-based RL"}),": Learn world model, plan in imagination"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Imitation learning"}),": Bootstrap from demonstrations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Curriculum learning"}),": Start easy, gradually increase difficulty"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"You've learned RL for training humanoid behaviors. Next, explore synthetic data generation for perception."}),"\n",(0,s.jsxs)(e.p,{children:["\ud83d\udc49 ",(0,s.jsx)(e.strong,{children:(0,s.jsx)(e.a,{href:"synthetic-data",children:"Next: Synthetic Data Generation \u2192"})})]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.admonition,{title:"RL Tips",type:"tip",children:(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Start simple"}),": Train basic behaviors first (balancing before walking)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tune reward function"}),": Most important factor for success"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Use domain randomization"}),": Essential for sim-to-real"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallelize"}),": Run 100s of environments for fast training"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Monitor training"}),": Use TensorBoard to debug"]}),"\n"]})}),"\n",(0,s.jsx)(e.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Train humanoid to walk forward"}),":"]}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Setup Isaac Gym"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Install Isaac Sim"}),"\n",(0,s.jsx)(e.li,{children:"Load humanoid environment"}),"\n",(0,s.jsx)(e.li,{children:"Configure observation and action spaces"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Define reward function"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reward forward velocity"}),"\n",(0,s.jsx)(e.li,{children:"Penalize falling"}),"\n",(0,s.jsx)(e.li,{children:"Penalize high energy"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Train with PPO"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"1024 parallel environments"}),"\n",(0,s.jsx)(e.li,{children:"Train for 1M timesteps"}),"\n",(0,s.jsx)(e.li,{children:"Monitor reward in TensorBoard"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Evaluate policy"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test in simulation"}),"\n",(0,s.jsx)(e.li,{children:"Measure: average velocity, success rate, energy efficiency"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Domain randomization"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Add mass randomization"}),"\n",(0,s.jsx)(e.li,{children:"Add friction randomization"}),"\n",(0,s.jsx)(e.li,{children:"Retrain and compare robustness"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Bonus"}),": Deploy trained policy to simulated humanoid in Gazebo."]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>a});var r=i(6540);const s={},o=r.createContext(s);function l(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:l(n.components),r.createElement(o.Provider,{value:e},n.children)}}}]);