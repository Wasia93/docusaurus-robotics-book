"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[6594],{8021:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"assessments/isaac-perception","title":"Assessment: Isaac-Based Perception Pipeline","description":"Overview","source":"@site/docs/assessments/isaac-perception.md","sourceDirName":"assessments","slug":"/assessments/isaac-perception","permalink":"/docusaurus-robotics-book/zh/assessments/isaac-perception","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Assessment: Gazebo Simulation Implementation","permalink":"/docusaurus-robotics-book/zh/assessments/gazebo-implementation"},"next":{"title":"Capstone Project - Autonomous Humanoid","permalink":"/docusaurus-robotics-book/zh/assessments/capstone"}}');var s=i(4848),r=i(8453);const o={},a="Assessment: Isaac-Based Perception Pipeline",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Requirements",id:"project-requirements",level:2},{value:"Task: Object Detection and Pose Estimation for Bin Picking",id:"task-object-detection-and-pose-estimation-for-bin-picking",level:3},{value:"1. Environment Setup",id:"1-environment-setup",level:4},{value:"Isaac Sim Scene Creation",id:"isaac-sim-scene-creation",level:5},{value:"2. Synthetic Data Generation",id:"2-synthetic-data-generation",level:4},{value:"Domain Randomization",id:"domain-randomization",level:5},{value:"3. Object Detection Model Training",id:"3-object-detection-model-training",level:4},{value:"YOLOv8 Training",id:"yolov8-training",level:5},{value:"4. ROS 2 Perception Node",id:"4-ros-2-perception-node",level:4},{value:"Detection Node",id:"detection-node",level:5},{value:"5. Point Cloud Processing",id:"5-point-cloud-processing",level:4},{value:"Segmentation and Pose Refinement",id:"segmentation-and-pose-refinement",level:5},{value:"Testing Requirements",id:"testing-requirements",level:2},{value:"1. Detection Accuracy",id:"1-detection-accuracy",level:3},{value:"2. Pose Estimation Accuracy",id:"2-pose-estimation-accuracy",level:3},{value:"3. Runtime Performance",id:"3-runtime-performance",level:3},{value:"4. Robustness",id:"4-robustness",level:3},{value:"Deliverables",id:"deliverables",level:2},{value:"1. Source Code",id:"1-source-code",level:3},{value:"2. Trained Model",id:"2-trained-model",level:3},{value:"3. Dataset",id:"3-dataset",level:3},{value:"4. Demonstration",id:"4-demonstration",level:3},{value:"5. Performance Report",id:"5-performance-report",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Tips for Success",id:"tips-for-success",level:2},{value:"Resources",id:"resources",level:2},{value:"Extension Challenges (Optional)",id:"extension-challenges-optional",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"assessment-isaac-based-perception-pipeline",children:"Assessment: Isaac-Based Perception Pipeline"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This assessment evaluates your ability to develop a complete perception pipeline using NVIDIA Isaac platform. You'll implement object detection, pose estimation, and scene understanding for robotic manipulation tasks."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this assessment, you will demonstrate:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Proficiency in Isaac Sim for synthetic data generation"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of Isaac ROS perception nodes"}),"\n",(0,s.jsx)(n.li,{children:"Ability to train and deploy vision models"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of 3D perception (depth, point clouds, pose estimation)"}),"\n",(0,s.jsx)(n.li,{children:"Integration of perception with robotic control"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"project-requirements",children:"Project Requirements"}),"\n",(0,s.jsx)(n.h3,{id:"task-object-detection-and-pose-estimation-for-bin-picking",children:"Task: Object Detection and Pose Estimation for Bin Picking"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete perception system that:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Detects objects in a cluttered bin"}),"\n",(0,s.jsx)(n.li,{children:"Estimates 6D pose (position + orientation)"}),"\n",(0,s.jsx)(n.li,{children:"Segments individual objects from point cloud"}),"\n",(0,s.jsx)(n.li,{children:"Publishes detections to ROS 2 for grasp planning"}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"1-environment-setup",children:"1. Environment Setup"}),"\n",(0,s.jsx)(n.h5,{id:"isaac-sim-scene-creation",children:"Isaac Sim Scene Creation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from omni.isaac.kit import SimulationApp\nsimulation_app = SimulationApp({"headless": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicCuboid, DynamicSphere, DynamicCylinder\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nimport numpy as np\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\n\n# Add bin\nadd_reference_to_stage(\n    usd_path="/Isaac/Props/Bins/bin_0.usd",\n    prim_path="/World/Bin"\n)\n\n# Populate bin with objects\nobjects = []\nfor i in range(15):\n    obj_type = np.random.choice([\'cube\', \'sphere\', \'cylinder\'])\n\n    if obj_type == \'cube\':\n        obj = world.scene.add(\n            DynamicCuboid(\n                prim_path=f"/World/Object_{i}",\n                name=f"object_{i}",\n                position=[np.random.uniform(-0.2, 0.2),\n                         np.random.uniform(-0.2, 0.2),\n                         0.5 + i * 0.15],\n                size=0.08,\n                color=np.random.rand(3)\n            )\n        )\n    elif obj_type == \'sphere\':\n        obj = world.scene.add(\n            DynamicSphere(\n                prim_path=f"/World/Object_{i}",\n                name=f"object_{i}",\n                position=[np.random.uniform(-0.2, 0.2),\n                         np.random.uniform(-0.2, 0.2),\n                         0.5 + i * 0.15],\n                radius=0.04,\n                color=np.random.rand(3)\n            )\n        )\n    # Add cylinder similarly\n\n    objects.append(obj)\n\n# Add camera\nfrom omni.isaac.sensor import Camera\ncamera = world.scene.add(\n    Camera(\n        prim_path="/World/Camera",\n        position=[0, -0.5, 0.8],\n        orientation=[0.7071, 0, 0.7071, 0],  # Looking at bin\n        resolution=(1280, 720),\n        frequency=30\n    )\n)\n'})}),"\n",(0,s.jsx)(n.h4,{id:"2-synthetic-data-generation",children:"2. Synthetic Data Generation"}),"\n",(0,s.jsx)(n.h5,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import omni.replicator.core as rep\n\n# Setup camera\ncamera_rep = rep.create.camera(\n    position=(0, -0.5, 0.8),\n    look_at=(0, 0, 0.3)\n)\n\nrender_product = rep.create.render_product(camera_rep, (1280, 720))\n\n# Randomization\nwith rep.trigger.on_frame(num_frames=1000):\n    # Randomize lighting\n    with rep.create.light(\n        light_type="Sphere",\n        intensity=rep.distribution.uniform(1000, 5000),\n        position=rep.distribution.uniform((-1, -1, 1), (1, 1, 2)),\n        scale=0.1\n    ) as light:\n        pass\n\n    # Randomize object positions\n    with rep.get.prims(semantics=[("class", "object")]) as objects:\n        rep.modify.pose(\n            position=rep.distribution.uniform(\n                (-0.25, -0.25, 0.3),\n                (0.25, 0.25, 0.8)\n            ),\n            rotation=rep.distribution.uniform(\n                (0, 0, 0),\n                (360, 360, 360)\n            )\n        )\n\n    # Randomize textures\n    with rep.get.prims(semantics=[("class", "object")]) as objects:\n        rep.randomizer.color(\n            colors=rep.distribution.uniform((0, 0, 0), (1, 1, 1))\n        )\n\n# Writers\nrgb_writer = rep.WriterRegistry.get("BasicWriter")\nrgb_writer.initialize(\n    output_dir="data/rgb",\n    rgb=True\n)\n\ndepth_writer = rep.WriterRegistry.get("BasicWriter")\ndepth_writer.initialize(\n    output_dir="data/depth",\n    distance_to_camera=True\n)\n\nbbox_writer = rep.WriterRegistry.get("BasicWriter")\nbbox_writer.initialize(\n    output_dir="data/bbox",\n    bounding_box_2d_tight=True\n)\n\nsemantic_writer = rep.WriterRegistry.get("BasicWriter")\nsemantic_writer.initialize(\n    output_dir="data/semantic",\n    semantic_segmentation=True\n)\n\n# Attach writers\nrgb_writer.attach([render_product])\ndepth_writer.attach([render_product])\nbbox_writer.attach([render_product])\nsemantic_writer.attach([render_product])\n\n# Run generation\nrep.orchestrator.run()\n'})}),"\n",(0,s.jsx)(n.h4,{id:"3-object-detection-model-training",children:"3. Object Detection Model Training"}),"\n",(0,s.jsx)(n.h5,{id:"yolov8-training",children:"YOLOv8 Training"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Prepare dataset in YOLO format\n# data.yaml:\n\"\"\"\ntrain: data/train/images\nval: data/val/images\n\nnc: 3  # number of classes\nnames: ['cube', 'sphere', 'cylinder']\n\"\"\"\n\n# Train model\nmodel = YOLO('yolov8n.pt')\n\nresults = model.train(\n    data='data/data.yaml',\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    name='bin_picking_detector'\n)\n\n# Validate\nmetrics = model.val()\n\n# Export to ONNX for inference\nmodel.export(format='onnx')\n"})}),"\n",(0,s.jsx)(n.h4,{id:"4-ros-2-perception-node",children:"4. ROS 2 Perception Node"}),"\n",(0,s.jsx)(n.h5,{id:"detection-node",children:"Detection Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection3DArray, Detection3D\nfrom geometry_msgs.msg import Pose, Point\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\nimport onnxruntime as ort\n\nclass ObjectDetectionNode(Node):\n    def __init__(self):\n        super().__init__('object_detection_node')\n\n        # Load model\n        self.session = ort.InferenceSession('bin_detector.onnx')\n\n        # Subscribers\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/rgb', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth', self.depth_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/camera_info', self.camera_info_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection3DArray, '/detections_3d', 10\n        )\n\n        self.bridge = CvBridge()\n        self.camera_matrix = None\n        self.latest_depth = None\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera intrinsics\"\"\"\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def depth_callback(self, msg):\n        \"\"\"Store latest depth image\"\"\"\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n\n    def rgb_callback(self, msg):\n        \"\"\"Detect objects and estimate poses\"\"\"\n\n        # Convert to OpenCV\n        rgb = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Run detection\n        detections_2d = self.detect_objects(rgb)\n\n        # Estimate 3D poses\n        if self.latest_depth is not None and self.camera_matrix is not None:\n            detections_3d = self.estimate_3d_poses(\n                detections_2d,\n                self.latest_depth,\n                self.camera_matrix\n            )\n\n            # Publish\n            detection_msg = self.create_detection_msg(detections_3d)\n            self.detection_pub.publish(detection_msg)\n\n    def detect_objects(self, image):\n        \"\"\"Run YOLO detection\"\"\"\n\n        # Preprocess\n        input_img = cv2.resize(image, (640, 640))\n        input_img = input_img.transpose(2, 0, 1)  # HWC to CHW\n        input_img = input_img.astype(np.float32) / 255.0\n        input_img = np.expand_dims(input_img, axis=0)\n\n        # Inference\n        outputs = self.session.run(None, {'images': input_img})\n\n        # Post-process\n        detections = self.post_process_yolo(outputs, image.shape[:2])\n\n        return detections\n\n    def estimate_3d_poses(self, detections_2d, depth_image, camera_matrix):\n        \"\"\"Estimate 3D pose from 2D detection and depth\"\"\"\n\n        detections_3d = []\n\n        for det in detections_2d:\n            # Get center point\n            cx = int(det['bbox'][0] + det['bbox'][2] / 2)\n            cy = int(det['bbox'][1] + det['bbox'][3] / 2)\n\n            # Get depth\n            depth = depth_image[cy, cx]\n\n            if depth > 0:\n                # Backproject to 3D\n                fx = camera_matrix[0, 0]\n                fy = camera_matrix[1, 1]\n                cx_cam = camera_matrix[0, 2]\n                cy_cam = camera_matrix[1, 2]\n\n                x = (cx - cx_cam) * depth / fx\n                y = (cy - cy_cam) * depth / fy\n                z = depth\n\n                # Estimate orientation (simplified)\n                orientation = self.estimate_orientation(\n                    det, depth_image, camera_matrix\n                )\n\n                detections_3d.append({\n                    'class': det['class'],\n                    'confidence': det['confidence'],\n                    'position': [x, y, z],\n                    'orientation': orientation\n                })\n\n        return detections_3d\n\n    def estimate_orientation(self, detection, depth_image, camera_matrix):\n        \"\"\"Estimate object orientation using PCA or similar\"\"\"\n\n        # Extract object point cloud from bounding box\n        bbox = detection['bbox']\n        object_depth = depth_image[\n            bbox[1]:bbox[1]+bbox[3],\n            bbox[0]:bbox[0]+bbox[2]\n        ]\n\n        # Backproject to 3D points\n        points_3d = self.depth_to_pointcloud(\n            object_depth, bbox, camera_matrix\n        )\n\n        # Fit orientation using PCA\n        if len(points_3d) > 10:\n            # Center points\n            centroid = np.mean(points_3d, axis=0)\n            centered = points_3d - centroid\n\n            # PCA\n            cov = np.cov(centered.T)\n            eigenvalues, eigenvectors = np.linalg.eig(cov)\n\n            # Largest eigenvector is main axis\n            main_axis = eigenvectors[:, np.argmax(eigenvalues)]\n\n            # Convert to quaternion\n            orientation = self.axis_to_quaternion(main_axis)\n\n            return orientation\n\n        return [0, 0, 0, 1]  # Identity quaternion\n"})}),"\n",(0,s.jsx)(n.h4,{id:"5-point-cloud-processing",children:"5. Point Cloud Processing"}),"\n",(0,s.jsx)(n.h5,{id:"segmentation-and-pose-refinement",children:"Segmentation and Pose Refinement"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import open3d as o3d\n\nclass PointCloudProcessor(Node):\n    def __init__(self):\n        super().__init__(\'pointcloud_processor\')\n\n        self.pcd_sub = self.create_subscription(\n            PointCloud2, \'/camera/points\', self.pcd_callback, 10\n        )\n\n    def pcd_callback(self, msg):\n        """Process point cloud for segmentation"""\n\n        # Convert ROS PointCloud2 to Open3D\n        pcd = self.ros_to_open3d(msg)\n\n        # Remove outliers\n        pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n\n        # Plane segmentation (remove table)\n        plane_model, inliers = pcd.segment_plane(\n            distance_threshold=0.01,\n            ransac_n=3,\n            num_iterations=1000\n        )\n\n        object_cloud = pcd.select_by_index(inliers, invert=True)\n\n        # Cluster objects\n        labels = np.array(\n            object_cloud.cluster_dbscan(eps=0.02, min_points=10)\n        )\n\n        # Process each cluster\n        for label in set(labels):\n            if label == -1:\n                continue\n\n            # Extract cluster\n            cluster_indices = np.where(labels == label)[0]\n            cluster = object_cloud.select_by_index(cluster_indices)\n\n            # Estimate pose using ICP or other methods\n            pose = self.estimate_cluster_pose(cluster)\n\n            # Publish detection\n            # ...\n\n    def estimate_cluster_pose(self, cluster):\n        """Refine pose estimation for cluster"""\n\n        # Compute oriented bounding box\n        obb = cluster.get_oriented_bounding_box()\n\n        position = obb.center\n        orientation = obb.R  # Rotation matrix\n\n        # Convert rotation matrix to quaternion\n        quat = self.rotation_matrix_to_quaternion(orientation)\n\n        return {\n            \'position\': position,\n            \'orientation\': quat\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing-requirements",children:"Testing Requirements"}),"\n",(0,s.jsx)(n.h3,{id:"1-detection-accuracy",children:"1. Detection Accuracy"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Measure precision, recall, F1 score"}),"\n",(0,s.jsx)(n.li,{children:"Test on held-out validation set"}),"\n",(0,s.jsx)(n.li,{children:"Minimum 90% detection rate for visible objects"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-pose-estimation-accuracy",children:"2. Pose Estimation Accuracy"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Compare estimated vs. ground truth poses"}),"\n",(0,s.jsx)(n.li,{children:"Position error < 5mm"}),"\n",(0,s.jsx)(n.li,{children:"Orientation error < 10 degrees"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-runtime-performance",children:"3. Runtime Performance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Detection + pose estimation < 100ms"}),"\n",(0,s.jsx)(n.li,{children:"Maintain 10 Hz perception loop"}),"\n",(0,s.jsx)(n.li,{children:"Monitor GPU/CPU usage"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-robustness",children:"4. Robustness"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Test under varying lighting conditions"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate with occlusions (10-30% occluded)"}),"\n",(0,s.jsx)(n.li,{children:"Test with novel object colors/textures"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deliverables",children:"Deliverables"}),"\n",(0,s.jsx)(n.h3,{id:"1-source-code",children:"1. Source Code"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"isaac_perception/\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 perception.launch.py\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 perception_params.yaml\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 generate_synthetic_data.py\n\u2502   \u251c\u2500\u2500 train_detector.py\n\u2502   \u2514\u2500\u2500 evaluate_model.py\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 detection_node.cpp/py\n\u2502   \u2514\u2500\u2500 pointcloud_processor.cpp/py\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 bin_detector.onnx\n\u2514\u2500\u2500 README.md\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-trained-model",children:"2. Trained Model"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Trained detection model (ONNX format)"}),"\n",(0,s.jsx)(n.li,{children:"Training logs and curves"}),"\n",(0,s.jsx)(n.li,{children:"Model performance metrics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-dataset",children:"3. Dataset"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"1000+ synthetic training images"}),"\n",(0,s.jsx)(n.li,{children:"Annotations (bounding boxes, segmentation masks)"}),"\n",(0,s.jsx)(n.li,{children:"Data generation scripts"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"4-demonstration",children:"4. Demonstration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Video showing:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object detection in Isaac Sim"}),"\n",(0,s.jsx)(n.li,{children:"3D pose visualization"}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 integration"}),"\n",(0,s.jsx)(n.li,{children:"Grasp planning based on detections"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"5-performance-report",children:"5. Performance Report"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Detection metrics (precision/recall)"}),"\n",(0,s.jsx)(n.li,{children:"Pose estimation accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Runtime benchmarks"}),"\n",(0,s.jsx)(n.li,{children:"Failure case analysis"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Criteria"}),(0,s.jsx)(n.th,{children:"Points"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Synthetic Data"})}),(0,s.jsx)(n.td,{children:"15"}),(0,s.jsx)(n.td,{children:"Quality and diversity of generated data"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Detection Model"})}),(0,s.jsx)(n.td,{children:"20"}),(0,s.jsx)(n.td,{children:"Accuracy and robustness"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"3D Pose Estimation"})}),(0,s.jsx)(n.td,{children:"25"}),(0,s.jsx)(n.td,{children:"Pose accuracy and reliability"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Point Cloud Processing"})}),(0,s.jsx)(n.td,{children:"15"}),(0,s.jsx)(n.td,{children:"Segmentation and refinement"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ROS 2 Integration"})}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"Clean interface, proper topics"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Performance"})}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"Real-time capable"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Documentation"})}),(0,s.jsx)(n.td,{children:"5"}),(0,s.jsx)(n.td,{children:"Clear documentation and code quality"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Total: 100 points"})}),"\n",(0,s.jsx)(n.h2,{id:"tips-for-success",children:"Tips for Success"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generate diverse data"}),": Vary lighting, object poses, backgrounds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validate incrementally"}),": Test 2D detection before 3D pose"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use visualization"}),": Visualize detections and point clouds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Benchmark performance"}),": Profile code to find bottlenecks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Handle edge cases"}),": Occluded objects, poor lighting"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/replicator_tutorials/",children:"Isaac Sim Replicator"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/",children:"Isaac ROS"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"YOLOv8 Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"http://www.open3d.org/docs/",children:"Open3D Tutorial"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"extension-challenges-optional",children:"Extension Challenges (Optional)"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"6D pose estimation"}),": Use PVNet or similar for exact pose"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-world transfer"}),": Test on real camera data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-view fusion"}),": Combine multiple camera views"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Online learning"}),": Adapt model during deployment"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Good luck with your perception pipeline!"})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);