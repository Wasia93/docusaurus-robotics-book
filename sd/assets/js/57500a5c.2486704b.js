"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[1833],{2009:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Overview","source":"@site/docs/module4-vla/cognitive-planning.md","sourceDirName":"module4-vla","slug":"/module4-vla/cognitive-planning","permalink":"/docusaurus-robotics-book/sd/module4-vla/cognitive-planning","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Integration in Robotics","permalink":"/docusaurus-robotics-book/sd/module4-vla/llm-robotics"},"next":{"title":"Humanoid Robot Kinematics","permalink":"/docusaurus-robotics-book/sd/module4-vla/humanoid-kinematics"}}');var o=a(4848),i=a(8453);const r={},s="Cognitive Planning with LLMs",l={},c=[{value:"Overview",id:"overview",level:2},{value:"From Commands to Actions",id:"from-commands-to-actions",level:2},{value:"The Planning Hierarchy",id:"the-planning-hierarchy",level:3},{value:"LLM as Task Planner",id:"llm-as-task-planner",level:2},{value:"Hierarchical Task Network (HTN) Planning",id:"hierarchical-task-network-htn-planning",level:3},{value:"Reasoning About Constraints",id:"reasoning-about-constraints",level:2},{value:"Spatial Reasoning",id:"spatial-reasoning",level:3},{value:"Temporal Reasoning",id:"temporal-reasoning",level:3},{value:"Chain-of-Thought Reasoning",id:"chain-of-thought-reasoning",level:2},{value:"Step-by-Step Problem Solving",id:"step-by-step-problem-solving",level:3},{value:"ReAct: Reasoning + Acting",id:"react-reasoning--acting",level:2},{value:"Interleaved Reasoning and Action",id:"interleaved-reasoning-and-action",level:3},{value:"Error Recovery and Replanning",id:"error-recovery-and-replanning",level:2},{value:"Handling Failures",id:"handling-failures",level:3},{value:"Example Failure Recovery",id:"example-failure-recovery",level:3},{value:"Tool Use and External Resources",id:"tool-use-and-external-resources",level:2},{value:"Integrating External Tools",id:"integrating-external-tools",level:3},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Task Decomposition",id:"exercise-1-task-decomposition",level:3},{value:"Exercise 2: ReAct Agent",id:"exercise-2-react-agent",level:3},{value:"Exercise 3: Error Recovery",id:"exercise-3-error-recovery",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Cognitive planning bridges high-level natural language commands with low-level robot actions. This module explores how LLMs can decompose complex tasks, reason about constraints, and generate executable plans."}),"\n",(0,o.jsx)(n.h2,{id:"from-commands-to-actions",children:"From Commands to Actions"}),"\n",(0,o.jsx)(n.h3,{id:"the-planning-hierarchy",children:"The Planning Hierarchy"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Natural Language \u2192 Task Decomposition \u2192 Motion Planning \u2192 Motor Commands\n"})}),"\n",(0,o.jsx)(n.p,{children:'Example: "Clean the room"'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Level"}),": Identify objects, plan sequence"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion Level"}),": Path planning, collision avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Control Level"}),": Joint trajectories, gripper control"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"llm-as-task-planner",children:"LLM as Task Planner"}),"\n",(0,o.jsx)(n.h3,{id:"hierarchical-task-network-htn-planning",children:"Hierarchical Task Network (HTN) Planning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def plan_task(task_description: str) -> dict:\n    """Use LLM to decompose task into subtasks"""\n\n    prompt = f"""\n    You are a robot task planner. Decompose the following task into\n    a hierarchical plan using these primitives:\n\n    Navigation: navigate_to(location), explore_area(region)\n    Perception: detect_objects(query), identify_object(id)\n    Manipulation: pick_object(id), place_object(location), push_object(id)\n    Interaction: open_door(id), close_gripper(), speak(text)\n\n    Task: {task_description}\n\n    Return a JSON hierarchical task network.\n    """\n\n    response = call_llm(prompt)\n    return json.loads(response)\n'})}),"\n",(0,o.jsx)(n.p,{children:'Example output for "Clean the room":'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n    "task": "clean_room",\n    "subtasks": [\n        {\n            "task": "identify_mess",\n            "actions": [\n                {"action": "navigate_to", "params": {"location": "room_center"}},\n                {"action": "explore_area", "params": {"region": "room"}},\n                {"action": "detect_objects", "params": {"query": "trash, dirty items"}}\n            ]\n        },\n        {\n            "task": "collect_items",\n            "repeat_for_each": "detected_object",\n            "actions": [\n                {"action": "navigate_to", "params": {"location": "object_location"}},\n                {"action": "pick_object", "params": {"id": "current_object"}},\n                {"action": "navigate_to", "params": {"location": "trash_bin"}},\n                {"action": "place_object", "params": {"location": "trash_bin"}}\n            ]\n        },\n        {\n            "task": "confirm_completion",\n            "actions": [\n                {"action": "speak", "params": {"text": "Room cleaning complete"}}\n            ]\n        }\n    ]\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"reasoning-about-constraints",children:"Reasoning About Constraints"}),"\n",(0,o.jsx)(n.h3,{id:"spatial-reasoning",children:"Spatial Reasoning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SpatialReasoner:\n    def __init__(self, semantic_map):\n        self.map = semantic_map\n\n    def reason_about_location(self, query: str) -> dict:\n        """Use LLM to reason about spatial relationships"""\n\n        context = f"""\n        Known locations and objects:\n        - Kitchen: [table, chair, refrigerator, sink]\n        - Living room: [couch, TV, bookshelf]\n        - Objects on kitchen table: [cup, plate, book]\n\n        Question: {query}\n\n        Provide reasoning and answer as JSON.\n        """\n\n        response = call_llm(context)\n        return json.loads(response)\n\n# Example\nreasoner.reason_about_location("Where should I place the cup after cleaning?")\n# Output: {"reasoning": "Cups belong in kitchen, preferably in cupboard or on drying rack",\n#          "location": "kitchen_cupboard", "alternative": "kitchen_counter"}\n'})}),"\n",(0,o.jsx)(n.h3,{id:"temporal-reasoning",children:"Temporal Reasoning"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def plan_with_temporal_constraints(goal: str, constraints: list) -> dict:\n    """Plan considering time and ordering constraints"""\n\n    prompt = f"""\n    Plan to achieve: {goal}\n\n    Constraints:\n    {format_constraints(constraints)}\n\n    Consider:\n    - Prerequisites (must happen before other actions)\n    - Duration estimates\n    - Parallel execution opportunities\n    - Resource conflicts\n\n    Return a PDDL-style temporal plan.\n    """\n\n    plan = call_llm(prompt)\n    return parse_temporal_plan(plan)\n'})}),"\n",(0,o.jsx)(n.p,{children:"Example constraints:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'constraints = [\n    "Must grasp object before placing it",\n    "Cannot navigate and manipulate simultaneously",\n    "Object detection takes 2-3 seconds",\n    "Navigation to kitchen takes ~10 seconds"\n]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"chain-of-thought-reasoning",children:"Chain-of-Thought Reasoning"}),"\n",(0,o.jsx)(n.h3,{id:"step-by-step-problem-solving",children:"Step-by-Step Problem Solving"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def reason_step_by_step(problem: str) -> dict:\n    """Use chain-of-thought prompting for complex reasoning"""\n\n    prompt = f"""\n    Problem: {problem}\n\n    Let\'s solve this step-by-step:\n    1. What do we know? (state the facts)\n    2. What is the goal? (clarify the objective)\n    3. What are the constraints? (identify limitations)\n    4. What are possible approaches? (brainstorm)\n    5. What is the best approach? (reason about tradeoffs)\n    6. What are the action steps? (concrete plan)\n\n    Think through each step carefully.\n    """\n\n    response = call_llm(prompt)\n    return parse_reasoning_chain(response)\n'})}),"\n",(0,o.jsx)(n.p,{children:'Example problem: "Retrieve the book from the high shelf without a ladder"'}),"\n",(0,o.jsx)(n.p,{children:"Response might include:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'1. Known: Book is on high shelf (2m), robot max reach is 1.5m, no ladder available\n2. Goal: Safely retrieve book\n3. Constraints: Cannot reach directly, must not damage book or shelf\n4. Approaches:\n   a. Use long object to push book off (risky - book might fall)\n   b. Stack stable objects to gain height (safer if objects available)\n   c. Request human assistance (most reliable)\n5. Best approach: Request human assistance - safest and most reliable\n6. Action: navigate_to(person), speak("Could you help me get the book from the high shelf?")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"react-reasoning--acting",children:"ReAct: Reasoning + Acting"}),"\n",(0,o.jsx)(n.h3,{id:"interleaved-reasoning-and-action",children:"Interleaved Reasoning and Action"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ReActAgent:\n    def __init__(self):\n        self.action_history = []\n        self.observations = []\n\n    def solve(self, goal: str):\n        """Interleave reasoning and acting until goal achieved"""\n\n        max_steps = 10\n        for step in range(max_steps):\n            # Reasoning step\n            thought = self.reason(goal)\n            self.get_logger().info(f"Thought: {thought}")\n\n            # Decide action\n            action = self.decide_action(thought)\n            self.get_logger().info(f"Action: {action}")\n\n            # Execute action\n            observation = self.execute(action)\n            self.observations.append(observation)\n            self.get_logger().info(f"Observation: {observation}")\n\n            # Check if goal achieved\n            if self.is_goal_achieved(goal, observation):\n                break\n\n    def reason(self, goal: str) -> str:\n        """LLM reasoning about current state and next action"""\n\n        context = f"""\n        Goal: {goal}\n\n        Previous actions: {self.action_history[-3:]}\n        Recent observations: {self.observations[-3:]}\n\n        Think: What should I do next to achieve the goal?\n        Consider: What went well? What failed? What to try?\n        """\n\n        return call_llm(context)\n'})}),"\n",(0,o.jsx)(n.p,{children:'Example ReAct trace for "Find and bring the red cup":'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Thought 1: I need to locate the red cup. I should explore the likely locations.\nAction 1: navigate_to("kitchen")\nObservation 1: Arrived at kitchen. Vision detects: [blue plate, green cup, red cup]\n\nThought 2: I found the red cup in the kitchen! Now I need to pick it up.\nAction 2: navigate_to(red_cup.location)\nObservation 2: Positioned in front of red cup\n\nThought 3: I\'m close enough to grasp the cup.\nAction 3: grasp_object("red_cup")\nObservation 3: Successfully grasped red cup\n\nThought 4: I have the cup, now return to user.\nAction 4: navigate_to("user_location")\nObservation 4: Arrived at user. Task complete.\n'})}),"\n",(0,o.jsx)(n.h2,{id:"error-recovery-and-replanning",children:"Error Recovery and Replanning"}),"\n",(0,o.jsx)(n.h3,{id:"handling-failures",children:"Handling Failures"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class RobustPlanner:\n    def execute_with_recovery(self, plan: list):\n        """Execute plan with error handling and replanning"""\n\n        for i, action in enumerate(plan):\n            try:\n                result = self.execute_action(action)\n\n                if not result.success:\n                    # Action failed, attempt recovery\n                    self.handle_failure(action, result, plan[i+1:])\n\n            except Exception as e:\n                self.get_logger().error(f"Exception during {action}: {e}")\n                self.emergency_replan(plan[i:])\n\n    def handle_failure(self, failed_action, result, remaining_plan):\n        """Use LLM to reason about failure and generate recovery"""\n\n        prompt = f"""\n        Action failed: {failed_action}\n        Failure reason: {result.error_message}\n        Remaining plan: {remaining_plan}\n\n        Analyze the failure and suggest:\n        1. Is retry likely to succeed?\n        2. Can we achieve the goal using an alternative approach?\n        3. Should we abort and report failure?\n\n        Provide recovery strategy.\n        """\n\n        recovery = call_llm(prompt)\n        return self.execute_recovery(recovery)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"example-failure-recovery",children:"Example Failure Recovery"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Failed action: grasp_object("cup")\nError: "Grasp failed - object slipped"\n\nLLM recovery reasoning:\n"The cup likely slipped due to smooth surface. Alternative approaches:\n1. Adjust gripper force and retry\n2. Grasp from different angle (e.g., handle if cup has one)\n3. Use two-handed grasp for better stability\n\nRecommendation: Try grasping from handle if detected, otherwise increase force and retry once."\n'})}),"\n",(0,o.jsx)(n.h2,{id:"tool-use-and-external-resources",children:"Tool Use and External Resources"}),"\n",(0,o.jsx)(n.h3,{id:"integrating-external-tools",children:"Integrating External Tools"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ToolUsingAgent:\n    def __init__(self):\n        self.tools = {\n            "calculator": self.use_calculator,\n            "map_search": self.search_map,\n            "object_database": self.query_objects,\n            "physics_sim": self.simulate_physics\n        }\n\n    def solve_with_tools(self, problem: str):\n        """LLM decides which tools to use"""\n\n        prompt = f"""\n        Available tools: {list(self.tools.keys())}\n\n        Problem: {problem}\n\n        Which tools do you need? How will you use them?\n        Provide step-by-step plan including tool usage.\n        """\n\n        plan = call_llm(prompt)\n        return self.execute_plan_with_tools(plan)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-1-task-decomposition",children:"Exercise 1: Task Decomposition"}),"\n",(0,o.jsx)(n.p,{children:"Implement hierarchical task planning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# TODO: Student implementation\n# 1. Create LLM prompt for task decomposition\n# 2. Parse hierarchical plan structure\n# 3. Implement subtask execution\n# 4. Test with: "Set the table for dinner"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"exercise-2-react-agent",children:"Exercise 2: ReAct Agent"}),"\n",(0,o.jsx)(n.p,{children:"Build a ReAct agent for dynamic problem solving:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# TODO: Student implementation\n# 1. Implement reasoning step (LLM call)\n# 2. Implement action execution\n# 3. Store and use observation history\n# 4. Test with: "Find the missing tool"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"exercise-3-error-recovery",children:"Exercise 3: Error Recovery"}),"\n",(0,o.jsx)(n.p,{children:"Add failure handling and replanning:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Detect action failures\n# 2. Use LLM to analyze failure\n# 3. Generate recovery strategy\n# 4. Test with: Intentionally failing grasp actions\n"})}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"LLMs excel at high-level task planning and reasoning"}),"\n",(0,o.jsx)(n.li,{children:"Chain-of-thought improves complex problem solving"}),"\n",(0,o.jsx)(n.li,{children:"ReAct pattern interleaves thinking and acting"}),"\n",(0,o.jsx)(n.li,{children:"Robust systems require error handling and replanning"}),"\n",(0,o.jsx)(n.li,{children:"Grounding and validation remain critical"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2210.03629",children:"ReAct: Reasoning and Acting Paper"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2201.11903",children:"Chain-of-Thought Prompting"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://robot-help.github.io/",children:"LLM-based Robot Planning"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["Continue to ",(0,o.jsx)(n.a,{href:"/docusaurus-robotics-book/sd/module4-vla/humanoid-kinematics",children:"Humanoid Kinematics"})," to learn about controlling humanoid robot motion."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>s});var t=a(6540);const o={},i=t.createContext(o);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);