"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[5506],{2089:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/voice-to-action","title":"Voice-to-Action Pipeline","description":"Using OpenAI Whisper and LLMs for voice-controlled robots","source":"@site/docs/module4-vla/voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/voice-to-action","permalink":"/docusaurus-robotics-book/ur/module4-vla/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Voice-to-Action Pipeline","description":"Using OpenAI Whisper and LLMs for voice-controlled robots","keywords":["voice commands","Whisper","speech-to-text","LLM","natural language","robotics"]},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning for Robotics","permalink":"/docusaurus-robotics-book/ur/module3-isaac/reinforcement-learning"},"next":{"title":"LLM Integration in Robotics","permalink":"/docusaurus-robotics-book/ur/module4-vla/llm-robotics"}}');var s=i(4848),t=i(8453);const r={sidebar_position:1,title:"Voice-to-Action Pipeline",description:"Using OpenAI Whisper and LLMs for voice-controlled robots",keywords:["voice commands","Whisper","speech-to-text","LLM","natural language","robotics"]},a="Module 4: Vision-Language-Action - Voice-to-Action",l={},c=[{value:"Introduction to VLA",id:"introduction-to-vla",level:2},{value:"The Voice-to-Action Pipeline",id:"the-voice-to-action-pipeline",level:2},{value:"Step 1: Audio Capture",id:"step-1-audio-capture",level:2},{value:"Using ReSpeaker Microphone",id:"using-respeaker-microphone",level:3},{value:"ROS 2 Audio Capture Node",id:"ros-2-audio-capture-node",level:3},{value:"Step 2: Speech-to-Text with Whisper",id:"step-2-speech-to-text-with-whisper",level:2},{value:"OpenAI Whisper",id:"openai-whisper",level:3},{value:"Installation",id:"installation",level:3},{value:"Whisper ROS 2 Node",id:"whisper-ros-2-node",level:3},{value:"Running Whisper Node",id:"running-whisper-node",level:3},{value:"Step 3: Language Understanding with LLMs",id:"step-3-language-understanding-with-llms",level:2},{value:"Using GPT-4 for Command Parsing",id:"using-gpt-4-for-command-parsing",level:3},{value:"Custom Robot Command Message",id:"custom-robot-command-message",level:3},{value:"Step 4: Action Planning",id:"step-4-action-planning",level:2},{value:"Task Planner Node",id:"task-planner-node",level:3},{value:"Step 5: Complete Voice Control System",id:"step-5-complete-voice-control-system",level:2},{value:"Launch File",id:"launch-file",level:3},{value:"Testing the Pipeline",id:"testing-the-pipeline",level:2},{value:"Test with Audio File",id:"test-with-audio-file",level:3},{value:"Test with Live Microphone",id:"test-with-live-microphone",level:3},{value:"Advanced: End-to-End VLA Models",id:"advanced-end-to-end-vla-models",level:2},{value:"RT-2 (Robotic Transformer 2)",id:"rt-2-robotic-transformer-2",level:3},{value:"Open-Source Alternatives",id:"open-source-alternatives",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Lab Exercise",id:"lab-exercise",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action---voice-to-action",children:"Module 4: Vision-Language-Action - Voice-to-Action"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-vla",children:"Introduction to VLA"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"})," models represent the convergence of:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision"}),": Understanding the physical world through cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": Natural language instructions from humans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Motor commands to execute tasks"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This module focuses on building a ",(0,s.jsx)(n.strong,{children:"voice-to-action pipeline"})," where:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human speaks"}),': "Pick up the red cup"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech-to-Text"}),": Transcribe audio to text (Whisper)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Understanding"}),": Parse command (LLM/GPT)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Planning"}),": Translate to robot actions (ROS 2 commands)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution"}),": Robot performs task"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"the-voice-to-action-pipeline",children:"The Voice-to-Action Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Human: "Move forward and pick up the red cup"          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  1. Audio Capture          \u2502  (Microphone)\n    \u2502     ReSpeaker Mic Array    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  2. Speech-to-Text         \u2502  (OpenAI Whisper)\n    \u2502     Transcribe audio       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  3. Language Understanding \u2502  (GPT-4)\n    \u2502     Parse intent & params  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  4. Action Planning        \u2502  (Task Planner)\n    \u2502     Sequence of actions    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  5. Robot Execution        \u2502  (ROS 2 Actions)\n    \u2502     Navigate, grasp, etc.  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-1-audio-capture",children:"Step 1: Audio Capture"}),"\n",(0,s.jsx)(n.h3,{id:"using-respeaker-microphone",children:"Using ReSpeaker Microphone"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hardware"}),": ReSpeaker USB Mic Array v2.0 ($69)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"4-microphone array"}),"\n",(0,s.jsx)(n.li,{children:"Far-field voice capture (3-5 meters)"}),"\n",(0,s.jsx)(n.li,{children:"Beamforming for noise reduction"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-audio-capture-node",children:"ROS 2 Audio Capture Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\nimport wave\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture')\n\n        # Audio parameters\n        self.CHUNK = 1024\n        self.FORMAT = pyaudio.paInt16\n        self.CHANNELS = 1\n        self.RATE = 16000  # 16kHz (Whisper's native rate)\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=self.FORMAT,\n            channels=self.CHANNELS,\n            rate=self.RATE,\n            input=True,\n            frames_per_buffer=self.CHUNK,\n        )\n\n        # Publisher for audio chunks\n        self.audio_pub = self.create_publisher(\n            String,\n            '/audio/raw',\n            10\n        )\n\n        # Timer for reading audio\n        self.timer = self.create_timer(0.1, self.capture_audio)\n\n        self.get_logger().info('Audio Capture Node started')\n\n    def capture_audio(self):\n        # Read audio chunk\n        data = self.stream.read(self.CHUNK, exception_on_overflow=False)\n\n        # Convert to numpy array\n        audio_data = np.frombuffer(data, dtype=np.int16)\n\n        # Check if speech detected (simple energy threshold)\n        energy = np.abs(audio_data).mean()\n        if energy > 500:  # Adjust threshold\n            self.get_logger().info(f'Speech detected, energy: {energy}')\n            # Publish audio (in practice, accumulate and publish full utterance)\n            # msg = String()\n            # msg.data = data.hex()\n            # self.audio_pub.publish(msg)\n\n    def destroy_node(self):\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n        super().destroy_node()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-2-speech-to-text-with-whisper",children:"Step 2: Speech-to-Text with Whisper"}),"\n",(0,s.jsx)(n.h3,{id:"openai-whisper",children:"OpenAI Whisper"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Whisper"})," is a robust speech recognition model:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"State-of-the-art accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Multilingual (99 languages)"}),"\n",(0,s.jsx)(n.li,{children:"Handles accents, noise, far-field"}),"\n",(0,s.jsx)(n.li,{children:"Open-source (MIT license)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n# OR for faster inference\npip install whisper-ctranslate2\n"})}),"\n",(0,s.jsx)(n.h3,{id:"whisper-ros-2-node",children:"Whisper ROS 2 Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport numpy as np\nimport wave\nimport tempfile\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__('whisper_node')\n\n        # Load Whisper model\n        self.declare_parameter('model_size', 'base')  # tiny, base, small, medium, large\n        model_size = self.get_parameter('model_size').value\n\n        self.get_logger().info(f'Loading Whisper model: {model_size}')\n        self.model = whisper.load_model(model_size)\n        self.get_logger().info('Whisper model loaded')\n\n        # Subscriber for audio\n        self.audio_sub = self.create_subscription(\n            String,\n            '/audio/utterance',\n            self.audio_callback,\n            10\n        )\n\n        # Publisher for transcriptions\n        self.text_pub = self.create_publisher(\n            String,\n            '/voice_command',\n            10\n        )\n\n    def audio_callback(self, msg):\n        # In practice, msg.data contains audio file path or audio bytes\n        audio_path = msg.data  # Path to .wav file\n\n        # Transcribe\n        self.get_logger().info('Transcribing audio...')\n        result = self.model.transcribe(\n            audio_path,\n            language='en',  # or auto-detect\n            fp16=False,  # Use FP32 (FP16 requires CUDA)\n        )\n\n        transcription = result['text'].strip()\n        self.get_logger().info(f'Transcription: \"{transcription}\"')\n\n        # Publish transcription\n        text_msg = String()\n        text_msg.data = transcription\n        self.text_pub.publish(text_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"running-whisper-node",children:"Running Whisper Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Launch Whisper node with base model\nros2 run voice_control whisper_node --ros-args -p model_size:=base\n\n# Test by publishing audio file path\nros2 topic pub /audio/utterance std_msgs/String \"data: '/path/to/command.wav'\"\n\n# Listen to transcriptions\nros2 topic echo /voice_command\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model sizes and performance"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"English-only"}),(0,s.jsx)(n.th,{children:"Multilingual"}),(0,s.jsx)(n.th,{children:"Speed (CPU)"}),(0,s.jsx)(n.th,{children:"Accuracy"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"39M"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"~10x real-time"}),(0,s.jsx)(n.td,{children:"Good"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"base"}),(0,s.jsx)(n.td,{children:"74M"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"~7x real-time"}),(0,s.jsx)(n.td,{children:"Better"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"244M"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"~4x real-time"}),(0,s.jsx)(n.td,{children:"Great"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"medium"}),(0,s.jsx)(n.td,{children:"769M"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"~2x real-time"}),(0,s.jsx)(n.td,{children:"Excellent"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"large"}),(0,s.jsx)(n.td,{children:"1550M"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"~1x real-time"}),(0,s.jsx)(n.td,{children:"Best"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recommendation"}),": ",(0,s.jsx)(n.strong,{children:"base"})," for real-time on Jetson, ",(0,s.jsx)(n.strong,{children:"small"})," for better accuracy."]}),"\n",(0,s.jsx)(n.h2,{id:"step-3-language-understanding-with-llms",children:"Step 3: Language Understanding with LLMs"}),"\n",(0,s.jsx)(n.h3,{id:"using-gpt-4-for-command-parsing",children:"Using GPT-4 for Command Parsing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom robot_interfaces.msg import RobotCommand  # Custom message\nimport openai\nimport os\nimport json\n\nclass LLMCommandParserNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_command_parser\')\n\n        # OpenAI API key\n        openai.api_key = os.getenv(\'OPENAI_API_KEY\')\n\n        # Subscribe to voice commands\n        self.voice_sub = self.create_subscription(\n            String,\n            \'/voice_command\',\n            self.command_callback,\n            10\n        )\n\n        # Publish parsed commands\n        self.command_pub = self.create_publisher(\n            RobotCommand,\n            \'/robot/command\',\n            10\n        )\n\n        # System prompt\n        self.system_prompt = """\n        You are a robot command parser. Convert natural language commands\n        into structured JSON with these fields:\n        - action: "navigate", "pick", "place", "stop", "wait"\n        - parameters: dict with action-specific params\n        - priority: "low", "medium", "high"\n\n        Examples:\n        Input: "Move forward 2 meters"\n        Output: {"action": "navigate", "parameters": {"direction": "forward", "distance": 2.0}, "priority": "medium"}\n\n        Input: "Pick up the red cup"\n        Output: {"action": "pick", "parameters": {"object": "cup", "color": "red"}, "priority": "high"}\n\n        Respond ONLY with valid JSON, no explanation.\n        """\n\n        self.get_logger().info(\'LLM Command Parser started\')\n\n    def command_callback(self, msg):\n        command_text = msg.data\n        self.get_logger().info(f\'Received command: "{command_text}"\')\n\n        # Query LLM\n        parsed_command = self.parse_command(command_text)\n\n        if parsed_command:\n            # Publish structured command\n            cmd_msg = RobotCommand()\n            cmd_msg.action = parsed_command[\'action\']\n            cmd_msg.parameters = json.dumps(parsed_command[\'parameters\'])\n            cmd_msg.priority = parsed_command[\'priority\']\n            self.command_pub.publish(cmd_msg)\n\n            self.get_logger().info(f\'Published command: {parsed_command["action"]}\')\n\n    def parse_command(self, text):\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": text}\n                ],\n                temperature=0.0,  # Deterministic\n                max_tokens=150,\n            )\n\n            content = response.choices[0].message.content.strip()\n\n            # Remove markdown code blocks if present\n            if content.startswith("```json"):\n                content = content.split("```json")[1].split("```")[0].strip()\n            elif content.startswith("```"):\n                content = content.split("```")[1].split("```")[0].strip()\n\n            # Parse JSON\n            parsed = json.loads(content)\n            return parsed\n\n        except Exception as e:\n            self.get_logger().error(f\'LLM parsing failed: {e}\')\n            return None\n'})}),"\n",(0,s.jsx)(n.h3,{id:"custom-robot-command-message",children:"Custom Robot Command Message"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"robot_interfaces/msg/RobotCommand.msg"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"string action\nstring parameters  # JSON string\nstring priority\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-4-action-planning",children:"Step 4: Action Planning"}),"\n",(0,s.jsx)(n.h3,{id:"task-planner-node",children:"Task Planner Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom robot_interfaces.msg import RobotCommand\nfrom geometry_msgs.msg import Twist\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport json\n\nclass TaskPlannerNode(Node):\n    def __init__(self):\n        super().__init__('task_planner')\n\n        # Subscribe to parsed commands\n        self.command_sub = self.create_subscription(\n            RobotCommand,\n            '/robot/command',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for various actions\n        self.vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        self.get_logger().info('Task Planner started')\n\n    def command_callback(self, msg):\n        action = msg.action\n        params = json.loads(msg.parameters)\n\n        self.get_logger().info(f'Executing action: {action} with params: {params}')\n\n        if action == \"navigate\":\n            self.execute_navigate(params)\n        elif action == \"pick\":\n            self.execute_pick(params)\n        elif action == \"stop\":\n            self.execute_stop()\n        else:\n            self.get_logger().warn(f'Unknown action: {action}')\n\n    def execute_navigate(self, params):\n        direction = params.get('direction', 'forward')\n        distance = params.get('distance', 1.0)\n\n        self.get_logger().info(f'Navigating {direction} for {distance}m')\n\n        # Simple velocity control\n        twist = Twist()\n        if direction == 'forward':\n            twist.linear.x = 0.5\n        elif direction == 'backward':\n            twist.linear.x = -0.5\n        elif direction == 'left':\n            twist.angular.z = 0.5\n        elif direction == 'right':\n            twist.angular.z = -0.5\n\n        # Publish for duration (simplified, use Nav2 in practice)\n        duration = distance / 0.5  # seconds\n        rate = self.create_rate(10)  # 10 Hz\n        for _ in range(int(duration * 10)):\n            self.vel_pub.publish(twist)\n            rate.sleep()\n\n        # Stop\n        self.vel_pub.publish(Twist())\n        self.get_logger().info('Navigation complete')\n\n    def execute_pick(self, params):\n        obj = params.get('object', 'unknown')\n        color = params.get('color', 'any')\n\n        self.get_logger().info(f'Picking {color} {obj}')\n\n        # 1. Use computer vision to detect object\n        # 2. Navigate to object\n        # 3. Lower arm and grasp\n        # 4. Lift object\n\n        # Placeholder for actual implementation\n        self.get_logger().warn('Pick action not fully implemented yet')\n\n    def execute_stop(self):\n        self.get_logger().info('Stopping robot')\n        twist = Twist()\n        self.vel_pub.publish(twist)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-5-complete-voice-control-system",children:"Step 5: Complete Voice Control System"}),"\n",(0,s.jsx)(n.h3,{id:"launch-file",children:"Launch File"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"voice_control.launch.py"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Audio capture\n        Node(\n            package='voice_control',\n            executable='audio_capture_node',\n            name='audio_capture',\n            output='screen',\n        ),\n\n        # Whisper speech-to-text\n        Node(\n            package='voice_control',\n            executable='whisper_node',\n            name='whisper',\n            parameters=[{'model_size': 'base'}],\n            output='screen',\n        ),\n\n        # LLM command parser\n        Node(\n            package='voice_control',\n            executable='llm_parser_node',\n            name='llm_parser',\n            output='screen',\n        ),\n\n        # Task planner\n        Node(\n            package='voice_control',\n            executable='task_planner_node',\n            name='task_planner',\n            output='screen',\n        ),\n    ])\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Run"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch voice_control voice_control.launch.py\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-the-pipeline",children:"Testing the Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"test-with-audio-file",children:"Test with Audio File"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Record test command\narecord -d 5 -f S16_LE -r 16000 test_command.wav\n\n# Publish to audio topic\nros2 topic pub /audio/utterance std_msgs/String \"data: '/path/to/test_command.wav'\"\n\n# Monitor transcription\nros2 topic echo /voice_command\n\n# Monitor parsed commands\nros2 topic echo /robot/command\n"})}),"\n",(0,s.jsx)(n.h3,{id:"test-with-live-microphone",children:"Test with Live Microphone"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Modify audio_capture_node to continuously listen\n# When energy threshold exceeded, record 3 seconds\n# Save to file and publish path to /audio/utterance\n"})}),"\n",(0,s.jsx)(n.h2,{id:"advanced-end-to-end-vla-models",children:"Advanced: End-to-End VLA Models"}),"\n",(0,s.jsx)(n.h3,{id:"rt-2-robotic-transformer-2",children:"RT-2 (Robotic Transformer 2)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Google DeepMind's RT-2"}),": Vision-Language-Action model"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Input: Image + text command"}),"\n",(0,s.jsx)(n.li,{children:"Output: Robot actions (joint positions)"}),"\n",(0,s.jsx)(n.li,{children:"Trained on web data + robot demonstrations"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Vision Encoder (ViT)                \u2502  \u2190 Camera image\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Language Model (PaLM/T5)            \u2502  \u2190 Text command\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Action Head (Transformer Decoder)   \u2502  \u2192 Robot actions\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Using RT-2"})," (hypothetical, model not publicly released):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Pseudocode (RT-2 not open-source yet)\nfrom rt2 import RT2Model\n\nmodel = RT2Model.load_pretrained("rt2-1x")\n\n# Input\nimage = capture_camera_image()  # numpy array\ncommand = "pick up the red cup"\n\n# Inference\nactions = model.predict(image, command)\n# actions: [joint1, joint2, ..., gripper]\n\n# Execute on robot\nrobot.set_joint_positions(actions)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"open-source-alternatives",children:"Open-Source Alternatives"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenVLA"})," (2024):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Open-source VLA model"}),"\n",(0,s.jsx)(n.li,{children:"Based on GPT-4V + action tokenization"}),"\n",(0,s.jsxs)(n.li,{children:["GitHub: ",(0,s.jsx)(n.a,{href:"https://github.com/openvla/openvla",children:"https://github.com/openvla/openvla"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"You've built a voice-to-action pipeline. Next, explore LLM-based cognitive planning for complex tasks."}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"llm-robotics",children:"Next: LLM Robotics Integration \u2192"})})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.admonition,{title:"Voice Control Tips",type:"tip",children:(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise reduction"}),": Use beamforming microphones (ReSpeaker)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Wake word"}),': Add "Hey robot" detection to avoid false triggers']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confidence threshold"}),": Only act on high-confidence transcriptions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),': Robot should confirm understanding ("Moving forward 2 meters")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),': Add "stop" command that interrupts all actions']}),"\n"]})}),"\n",(0,s.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Build a voice-controlled navigator"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Setup"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Install Whisper (base model)"}),"\n",(0,s.jsx)(n.li,{children:"Set up OpenAI API key"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Implement nodes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Audio capture (simple version: record on button press)"}),"\n",(0,s.jsx)(n.li,{children:"Whisper transcription"}),"\n",(0,s.jsx)(n.li,{children:"LLM parsing"}),"\n",(0,s.jsx)(n.li,{children:"Simple navigation (forward, backward, left, right)"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Test commands"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"Move forward 2 meters"'}),"\n",(0,s.jsx)(n.li,{children:'"Turn left 90 degrees"'}),"\n",(0,s.jsx)(n.li,{children:'"Go backward"'}),"\n",(0,s.jsx)(n.li,{children:'"Stop"'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Verify"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Transcription accuracy (>90%)"}),"\n",(0,s.jsx)(n.li,{children:"Command parsing correctness"}),"\n",(0,s.jsx)(n.li,{children:"Robot executes commands"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Bonus"}),': Add object detection + "Go to the red cube" command.']})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const s={},t=o.createContext(s);function r(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);