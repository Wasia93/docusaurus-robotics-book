"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[7535],{3080:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module4-vla/capstone-project","title":"Capstone Project: The Autonomous Humanoid","description":"Overview","source":"@site/docs/module4-vla/capstone-project.md","sourceDirName":"module4-vla","slug":"/module4-vla/capstone-project","permalink":"/docusaurus-robotics-book/ur/module4-vla/capstone-project","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Human-Robot Interaction Design","permalink":"/docusaurus-robotics-book/ur/module4-vla/hri-design"},"next":{"title":"Workstation Requirements","permalink":"/docusaurus-robotics-book/ur/hardware/workstation-requirements"}}');var t=i(4848),s=i(8453);const r={},a="Capstone Project: The Autonomous Humanoid",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Project Objective",id:"project-objective",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Project Phases",id:"project-phases",level:2},{value:"Phase 1: Environment Setup (Week 1)",id:"phase-1-environment-setup-week-1",level:3},{value:"Tasks",id:"tasks",level:4},{value:"Phase 2: Core Systems (Week 2-3)",id:"phase-2-core-systems-week-2-3",level:3},{value:"2.1 Vision &amp; Perception",id:"21-vision--perception",level:4},{value:"2.2 Navigation",id:"22-navigation",level:4},{value:"2.3 Manipulation",id:"23-manipulation",level:4},{value:"Phase 3: VLA Integration (Week 4)",id:"phase-3-vla-integration-week-4",level:3},{value:"Example Task Scenarios",id:"example-task-scenarios",level:2},{value:"Scenario 1: &quot;Bring me the red tool&quot;",id:"scenario-1-bring-me-the-red-tool",level:3},{value:"Scenario 2: &quot;Clean up the workspace&quot;",id:"scenario-2-clean-up-the-workspace",level:3},{value:"Scenario 3: &quot;Show me where the fire extinguisher is&quot;",id:"scenario-3-show-me-where-the-fire-extinguisher-is",level:3},{value:"Deliverables",id:"deliverables",level:2},{value:"Required Components",id:"required-components",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Tips for Success",id:"tips-for-success",level:2},{value:"Advanced Extensions (Optional)",id:"advanced-extensions-optional",level:2},{value:"Resources",id:"resources",level:2},{value:"Final Words",id:"final-words",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project integrates everything you've learned into a complete autonomous humanoid robot system. Your robot will receive voice commands, plan actions, navigate environments, identify objects, and manipulate them\u2014all while interacting naturally with humans."}),"\n",(0,t.jsx)(n.h2,{id:"project-objective",children:"Project Objective"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Create a simulated humanoid robot that can:"})}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand"})," natural language voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plan"})," task sequences using LLM reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigate"})," environments while avoiding obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perceive"})," objects and scene understanding using vision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulate"})," objects with grasping and placement"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Interact"})," naturally with humans using speech and gestures"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   User Voice Command                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Speech-to-Text \u2502  (OpenAI Whisper)\n         \u2502   (Whisper)     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   LLM Planner   \u2502  (GPT-4 / Claude)\n         \u2502  Task Reasoning \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 Action Executor \u2502\n         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n              \u2502   \u2502   \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502             \u2502             \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n\u2502Navigate\u2502   \u2502Perceive\u2502   \u2502Manipul.\u2502\n\u2502 Nav2 + \u2502   \u2502 Isaac  \u2502   \u2502MoveIt2 \u2502\n\u2502 VSLAM  \u2502   \u2502  ROS   \u2502   \u2502  IK    \u2502\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502             \u2502             \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n         \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   ROS 2  \u2502\n         \u2502Middleware\u2502\n         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n         \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Isaac   \u2502\n         \u2502   Sim    \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h2,{id:"project-phases",children:"Project Phases"}),"\n",(0,t.jsx)(n.h3,{id:"phase-1-environment-setup-week-1",children:"Phase 1: Environment Setup (Week 1)"}),"\n",(0,t.jsx)(n.h4,{id:"tasks",children:"Tasks"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Set up Isaac Sim environment"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Create warehouse environment\nfrom omni.isaac.kit import SimulationApp\nsimulation_app = SimulationApp({"headless": False})\n\nfrom omni.isaac.core import World\nworld = World(stage_units_in_meters=1.0)\n\n# Load warehouse scene\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nadd_reference_to_stage(\n    usd_path="/Isaac/Environments/Simple_Warehouse/warehouse.usd",\n    prim_path="/World/Warehouse"\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Load humanoid robot"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from omni.isaac.core.articulations import Articulation\n\nrobot = world.scene.add(\n    Articulation(\n        prim_path="/World/Humanoid",\n        name="my_humanoid",\n        position=[0, 0, 1.0]\n    )\n)\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Place objects in scene"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Add objects: boxes, tools, containers\nfrom omni.isaac.core.objects import DynamicCuboid\n\nobjects = []\nfor i in range(5):\n    obj = world.scene.add(\n        DynamicCuboid(\n            prim_path=f"/World/Object_{i}",\n            name=f"object_{i}",\n            position=[i * 0.5, 2.0, 0.5],\n            size=0.1,\n            color=np.random.rand(3)\n        )\n    )\n    objects.append(obj)\n'})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"phase-2-core-systems-week-2-3",children:"Phase 2: Core Systems (Week 2-3)"}),"\n",(0,t.jsx)(n.h4,{id:"21-vision--perception",children:"2.1 Vision & Perception"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\n\nclass PerceptionNode(Node):\n    def __init__(self):\n        super().__init__('perception_node')\n\n        # Subscribe to camera\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/rgb', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth', self.depth_callback, 10\n        )\n\n        # Publish detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detections', 10\n        )\n\n        # Load detection model\n        self.detector = load_object_detector()\n\n    def rgb_callback(self, msg):\n        # Convert ROS image to numpy\n        image = self.ros_to_cv2(msg)\n\n        # Run object detection\n        detections = self.detector.detect(image)\n\n        # Publish detections\n        detection_msg = self.create_detection_msg(detections)\n        self.detection_pub.publish(detection_msg)\n\n    def identify_object(self, detection, rgb, depth):\n        \"\"\"Identify object and compute 3D pose\"\"\"\n\n        # Extract object region\n        bbox = detection.bbox\n        object_rgb = rgb[bbox.y:bbox.y+bbox.h, bbox.x:bbox.x+bbox.w]\n        object_depth = depth[bbox.y:bbox.y+bbox.h, bbox.x:bbox.x+bbox.w]\n\n        # Compute 3D position\n        position_3d = self.depth_to_3d(\n            bbox.center_x, bbox.center_y,\n            np.median(object_depth)\n        )\n\n        return {\n            'class': detection.class_name,\n            'position': position_3d,\n            'confidence': detection.confidence\n        }\n"})}),"\n",(0,t.jsx)(n.h4,{id:"22-navigation",children:"2.2 Navigation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class NavigationController(Node):\n    def __init__(self):\n        super().__init__(\'navigation_controller\')\n\n        # Nav2 simple commander\n        self.navigator = BasicNavigator()\n\n        # Subscribe to goal poses\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/goal_pose\', self.goal_callback, 10\n        )\n\n    def navigate_to(self, target_position):\n        """Navigate to target position"""\n\n        goal_pose = PoseStamped()\n        goal_pose.header.frame_id = \'map\'\n        goal_pose.pose.position.x = target_position[0]\n        goal_pose.pose.position.y = target_position[1]\n        goal_pose.pose.orientation.w = 1.0\n\n        # Send goal to Nav2\n        self.navigator.goToPose(goal_pose)\n\n        # Wait for completion\n        while not self.navigator.isTaskComplete():\n            feedback = self.navigator.getFeedback()\n            self.get_logger().info(\n                f"Distance remaining: {feedback.distance_remaining:.2f}m"\n            )\n\n        result = self.navigator.getResult()\n        if result == TaskResult.SUCCEEDED:\n            self.get_logger().info("Navigation succeeded!")\n        else:\n            self.get_logger().error("Navigation failed!")\n'})}),"\n",(0,t.jsx)(n.h4,{id:"23-manipulation",children:"2.3 Manipulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ManipulationController(Node):\n    def __init__(self):\n        super().__init__(\'manipulation_controller\')\n\n        # MoveIt2 interface\n        self.moveit = MoveGroupInterface("arm", "robot_description")\n\n    def pick_object(self, object_pose):\n        """Pick up object at given pose"""\n\n        # 1. Move to pre-grasp pose\n        pre_grasp = object_pose.copy()\n        pre_grasp.position.z += 0.1  # 10cm above object\n\n        self.moveit.set_pose_target(pre_grasp)\n        self.moveit.go()\n\n        # 2. Open gripper\n        self.set_gripper(open=True)\n\n        # 3. Move to grasp pose\n        self.moveit.set_pose_target(object_pose)\n        self.moveit.go()\n\n        # 4. Close gripper\n        self.set_gripper(open=False)\n\n        # 5. Lift object\n        lift_pose = object_pose.copy()\n        lift_pose.position.z += 0.15\n        self.moveit.set_pose_target(lift_pose)\n        self.moveit.go()\n\n    def place_object(self, target_pose):\n        """Place held object at target pose"""\n\n        # 1. Move to pre-place pose\n        pre_place = target_pose.copy()\n        pre_place.position.z += 0.1\n\n        self.moveit.set_pose_target(pre_place)\n        self.moveit.go()\n\n        # 2. Move to place pose\n        self.moveit.set_pose_target(target_pose)\n        self.moveit.go()\n\n        # 3. Open gripper\n        self.set_gripper(open=True)\n\n        # 4. Retract\n        self.moveit.set_pose_target(pre_place)\n        self.moveit.go()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phase-3-vla-integration-week-4",children:"Phase 3: VLA Integration (Week 4)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VLAController(Node):\n    """Vision-Language-Action Controller"""\n\n    def __init__(self):\n        super().__init__(\'vla_controller\')\n\n        # Components\n        self.whisper = whisper.load_model("base")\n        self.llm_client = anthropic.Client()\n        self.perception = PerceptionNode()\n        self.navigation = NavigationController()\n        self.manipulation = ManipulationController()\n\n        # Semantic map\n        self.semantic_map = SemanticMap()\n\n    def process_voice_command(self, audio_file):\n        """Full pipeline: voice \u2192 action"""\n\n        # 1. Speech-to-Text\n        text = self.whisper.transcribe(audio_file)["text"]\n        self.get_logger().info(f"Command: {text}")\n\n        # 2. LLM Planning\n        plan = self.llm_plan(text)\n        self.get_logger().info(f"Plan: {plan}")\n\n        # 3. Execute plan\n        self.execute_plan(plan)\n\n    def llm_plan(self, command):\n        """Use LLM to plan action sequence"""\n\n        # Get current scene information\n        scene_description = self.get_scene_description()\n\n        prompt = f"""\n        You are controlling a humanoid robot in a warehouse.\n\n        Current scene:\n        {scene_description}\n\n        User command: "{command}"\n\n        Available actions:\n        - navigate_to(location_name)\n        - find_object(description)\n        - pick_object(object_id)\n        - place_object(location)\n        - speak(text)\n\n        Generate a JSON action sequence to accomplish the command.\n        """\n\n        response = self.llm_client.messages.create(\n            model="claude-3-5-sonnet-20241022",\n            messages=[{"role": "user", "content": prompt}],\n            max_tokens=1024\n        )\n\n        plan = json.loads(response.content[0].text)\n        return plan\n\n    def execute_plan(self, plan):\n        """Execute planned action sequence"""\n\n        for action in plan[\'actions\']:\n            action_type = action[\'type\']\n            params = action[\'params\']\n\n            if action_type == \'navigate_to\':\n                location = self.semantic_map.get_location(params[\'location\'])\n                self.navigation.navigate_to(location)\n\n            elif action_type == \'find_object\':\n                obj = self.perception.find_object(params[\'description\'])\n                if obj:\n                    self.get_logger().info(f"Found {obj[\'class\']} at {obj[\'position\']}")\n\n            elif action_type == \'pick_object\':\n                obj_pose = self.perception.get_object_pose(params[\'object_id\'])\n                self.manipulation.pick_object(obj_pose)\n\n            elif action_type == \'place_object\':\n                target = self.semantic_map.get_location(params[\'location\'])\n                self.manipulation.place_object(target)\n\n            elif action_type == \'speak\':\n                self.speak(params[\'text\'])\n\n            # Wait for action to complete\n            time.sleep(1.0)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"example-task-scenarios",children:"Example Task Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"scenario-1-bring-me-the-red-tool",children:'Scenario 1: "Bring me the red tool"'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'1. Speech recognition: "Bring me the red tool"\n2. LLM planning:\n   - find_object("red tool")\n   - navigate_to(object_location)\n   - pick_object(object_id)\n   - navigate_to("user")\n   - place_object("user_hand")\n   - speak("Here is the red tool")\n3. Execution with vision, navigation, manipulation\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-2-clean-up-the-workspace",children:'Scenario 2: "Clean up the workspace"'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'1. Speech recognition: "Clean up the workspace"\n2. LLM planning:\n   - navigate_to("workspace")\n   - find_objects("clutter, trash, misplaced items")\n   - for each object:\n       - pick_object(object)\n       - determine_destination(object_type)\n       - navigate_to(destination)\n       - place_object(destination)\n   - speak("Workspace is clean")\n3. Execution loop for multiple objects\n'})}),"\n",(0,t.jsx)(n.h3,{id:"scenario-3-show-me-where-the-fire-extinguisher-is",children:'Scenario 3: "Show me where the fire extinguisher is"'}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'1. Speech recognition: "Show me where the fire extinguisher is"\n2. LLM planning:\n   - find_object("fire extinguisher")\n   - navigate_to(fire_extinguisher_location)\n   - gesture("point" toward extinguisher)\n   - speak("The fire extinguisher is here")\n3. Execution with navigation and gestures\n'})}),"\n",(0,t.jsx)(n.h2,{id:"deliverables",children:"Deliverables"}),"\n",(0,t.jsx)(n.h3,{id:"required-components",children:"Required Components"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Demonstration Video (5-10 minutes)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Show all major capabilities"}),"\n",(0,t.jsx)(n.li,{children:"Include voice command examples"}),"\n",(0,t.jsx)(n.li,{children:"Show both successes and failure recovery"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Technical Report"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System architecture diagram"}),"\n",(0,t.jsx)(n.li,{children:"Component descriptions"}),"\n",(0,t.jsx)(n.li,{children:"Algorithm choices and rationale"}),"\n",(0,t.jsx)(n.li,{children:"Performance evaluation"}),"\n",(0,t.jsx)(n.li,{children:"Challenges and solutions"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Source Code"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Well-commented ROS 2 packages"}),"\n",(0,t.jsx)(n.li,{children:"Launch files for full system"}),"\n",(0,t.jsx)(n.li,{children:"README with setup instructions"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Live Demonstration"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Real-time execution in Isaac Sim"}),"\n",(0,t.jsx)(n.li,{children:"Handle 3-5 different commands"}),"\n",(0,t.jsx)(n.li,{children:"Q&A session"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Criteria"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Voice Understanding"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Accuracy, robustness to variations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Task Planning"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Reasoning quality, plan correctness"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Navigation"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Collision-free, efficiency"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Perception"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Object detection accuracy, 3D pose estimation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Manipulation"})}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Grasp success rate, placement precision"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Integration"})}),(0,t.jsx)(n.td,{children:"10%"}),(0,t.jsx)(n.td,{children:"System coherence, failure recovery"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"HRI"})}),(0,t.jsx)(n.td,{children:"10%"}),(0,t.jsx)(n.td,{children:"Naturalness, responsiveness, safety"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"tips-for-success",children:"Tips for Success"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Start Simple"}),": Begin with single actions, then combine"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test Incrementally"}),": Verify each component before integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle Failures"}),": Implement error recovery for common failures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use Simulation"}),": Test extensively in simulation before any real robot"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Document"}),": Keep notes on challenges and solutions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Iterate"}),": Refine based on testing results"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"advanced-extensions-optional",children:"Advanced Extensions (Optional)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-robot coordination"}),": Multiple robots working together"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning from demonstration"}),": Teach new tasks by showing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic environments"}),": Handle moving objects and people"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Long-horizon tasks"}),": Complex multi-step tasks spanning minutes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sim-to-real"}),": Deploy to actual hardware"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/examples/humanoid-capstone",children:"Example Capstone Projects"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/latest/tutorials.html",children:"Isaac Sim Tutorials"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"ROS 2 Integration Guide"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"final-words",children:"Final Words"}),"\n",(0,t.jsx)(n.p,{children:"This capstone project represents the culmination of your journey through Physical AI and Humanoid Robotics. You've learned the foundational technologies\u2014ROS 2, simulation, perception, planning, manipulation\u2014and now you're integrating them into a complete intelligent system."}),"\n",(0,t.jsx)(n.p,{children:"The future of robotics lies in systems that can understand human intent, reason about their environment, and act autonomously yet safely. Your capstone robot embodies this vision."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Good luck, and enjoy building the future of robotics!"})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);