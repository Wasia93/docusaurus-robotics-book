"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[8841],{8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(n){const e=r.useContext(o);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),r.createElement(o.Provider,{value:e},n.children)}},8983:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"part3-advanced/reinforcement-learning","title":"Reinforcement Learning Applications","description":"RL algorithms for robotic control and decision-making","source":"@site/docs/part3-advanced/02-reinforcement-learning.md","sourceDirName":"part3-advanced","slug":"/part3-advanced/reinforcement-learning","permalink":"/docusaurus-robotics-book/ps/part3-advanced/reinforcement-learning","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Reinforcement Learning Applications","description":"RL algorithms for robotic control and decision-making","keywords":["reinforcement learning","RL","PPO","SAC","policy gradient","robot learning"]}}');var t=i(4848),o=i(8453);const s={sidebar_position:2,title:"Reinforcement Learning Applications",description:"RL algorithms for robotic control and decision-making",keywords:["reinforcement learning","RL","PPO","SAC","policy gradient","robot learning"]},a="Reinforcement Learning Applications",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Content Coming Soon",id:"content-coming-soon",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"reinforcement-learning-applications",children:"Reinforcement Learning Applications"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Estimated Reading Time:"})," 30 minutes\n",(0,t.jsx)(e.strong,{children:"Difficulty:"})," Advanced\n",(0,t.jsx)(e.strong,{children:"Prerequisites:"})," Part 1-2, Machine learning, Python"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand RL fundamentals (MDP, value functions, policies)"}),"\n",(0,t.jsx)(e.li,{children:"Implement PPO and SAC for robot control"}),"\n",(0,t.jsx)(e.li,{children:"Handle sample efficiency challenges"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate RL policies safely"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"content-coming-soon",children:"Content Coming Soon"}),"\n",(0,t.jsx)(e.p,{children:"This chapter will cover RL algorithms (PPO, SAC, TD3), reward shaping, sample efficiency, and safe RL for physical robots."}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.p,{children:["\ud83d\udca1 ",(0,t.jsx)(e.strong,{children:"Pro Tip"}),": Always train RL agents in simulation first with domain randomization for better real-world transfer."]})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);