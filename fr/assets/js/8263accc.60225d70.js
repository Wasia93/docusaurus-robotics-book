"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[6258],{2638:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/hri-design","title":"Human-Robot Interaction Design","description":"Overview","source":"@site/docs/module4-vla/hri-design.md","sourceDirName":"module4-vla","slug":"/module4-vla/hri-design","permalink":"/docusaurus-robotics-book/fr/module4-vla/hri-design","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Grasping","permalink":"/docusaurus-robotics-book/fr/module4-vla/manipulation-grasping"},"next":{"title":"Capstone Project: The Autonomous Humanoid","permalink":"/docusaurus-robotics-book/fr/module4-vla/capstone-project"}}');var o=i(4848),s=i(8453);const r={},a="Human-Robot Interaction Design",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Principles of HRI",id:"principles-of-hri",level:2},{value:"Key Considerations",id:"key-considerations",level:3},{value:"Uncanny Valley",id:"uncanny-valley",level:3},{value:"Interaction Modalities",id:"interaction-modalities",level:2},{value:"1. Speech and Voice",id:"1-speech-and-voice",level:3},{value:"2. Gesture Recognition",id:"2-gesture-recognition",level:3},{value:"3. Facial Expression Recognition",id:"3-facial-expression-recognition",level:3},{value:"4. Gaze and Attention",id:"4-gaze-and-attention",level:3},{value:"Social Robot Behaviors",id:"social-robot-behaviors",level:2},{value:"Proxemics (Personal Space)",id:"proxemics-personal-space",level:3},{value:"Turn-Taking and Interruption Handling",id:"turn-taking-and-interruption-handling",level:3},{value:"Expressiveness and Body Language",id:"expressiveness-and-body-language",level:3},{value:"Safety in HRI",id:"safety-in-hri",level:2},{value:"Collision Avoidance",id:"collision-avoidance",level:3},{value:"Multimodal Interaction",id:"multimodal-interaction",level:2},{value:"Combining Modalities",id:"combining-modalities",level:3},{value:"Practical Exercises",id:"practical-exercises",level:2},{value:"Exercise 1: Conversational Robot",id:"exercise-1-conversational-robot",level:3},{value:"Exercise 2: Gesture Control",id:"exercise-2-gesture-control",level:3},{value:"Exercise 3: Social Navigation",id:"exercise-3-social-navigation",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"human-robot-interaction-design",children:"Human-Robot Interaction Design"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Human-Robot Interaction (HRI) design focuses on creating natural, intuitive, and safe interactions between humans and humanoid robots. This module covers principles of HRI, interaction modalities, and social robot behaviors."}),"\n",(0,o.jsx)(n.h2,{id:"principles-of-hri",children:"Principles of HRI"}),"\n",(0,o.jsx)(n.h3,{id:"key-considerations",children:"Key Considerations"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Naturalness"}),": Interactions should feel intuitive"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transparency"}),": Robot intentions should be clear"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Physical and psychological safety"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Predictability"}),": Humans should understand robot behavior"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Adaptability"}),": Respond to different users and contexts"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"uncanny-valley",children:"Uncanny Valley"}),"\n",(0,o.jsx)(n.p,{children:"The emotional response to human-like robots:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Emotional Response\n     ^\n     |        /\\\n     |       /  \\\n     |      /    \\___\n     |_____/         \\____\n     +--------------------\x3e Human Likeness\n           ^\n      Uncanny Valley\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Design Implication"}),': Avoid "almost human" appearance; either clearly robotic or highly realistic.']}),"\n",(0,o.jsx)(n.h2,{id:"interaction-modalities",children:"Interaction Modalities"}),"\n",(0,o.jsx)(n.h3,{id:"1-speech-and-voice",children:"1. Speech and Voice"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyttsx3\n\nclass VoiceInteraction:\n    def __init__(self):\n        self.speech_recognizer = whisper.load_model("base")\n        self.tts_engine = pyttsx3.init()\n\n    def listen(self, audio_file):\n        """Convert speech to text"""\n        result = self.speech_recognizer.transcribe(audio_file)\n        return result["text"]\n\n    def speak(self, text):\n        """Convert text to speech"""\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n\n    def conversation_loop(self):\n        """Interactive conversation"""\n        while True:\n            # Listen\n            user_input = self.listen("audio.wav")\n            print(f"User: {user_input}")\n\n            # Process with LLM\n            response = self.generate_response(user_input)\n            print(f"Robot: {response}")\n\n            # Speak\n            self.speak(response)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"2-gesture-recognition",children:"2. Gesture Recognition"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import mediapipe as mp\n\nclass GestureRecognizer:\n    def __init__(self):\n        self.hands = mp.solutions.hands.Hands()\n\n    def recognize_gesture(self, image):\n        """Detect hand gestures from camera image"""\n\n        results = self.hands.process(image)\n\n        if not results.multi_hand_landmarks:\n            return None\n\n        # Get hand landmarks\n        hand_landmarks = results.multi_hand_landmarks[0]\n\n        # Classify gesture\n        gesture = self.classify_hand_pose(hand_landmarks)\n\n        return gesture\n\n    def classify_hand_pose(self, landmarks):\n        """Classify hand gesture from landmarks"""\n\n        # Simple gesture recognition\n        # Check finger states (extended or bent)\n\n        fingers_extended = []\n        for finger in [\'thumb\', \'index\', \'middle\', \'ring\', \'pinky\']:\n            extended = self.is_finger_extended(landmarks, finger)\n            fingers_extended.append(extended)\n\n        # Map to gestures\n        if fingers_extended == [False, True, False, False, False]:\n            return "pointing"\n        elif fingers_extended == [True, False, False, False, True]:\n            return "rock_sign"\n        elif sum(fingers_extended) == 5:\n            return "open_hand"\n        elif sum(fingers_extended) == 0:\n            return "fist"\n        else:\n            return "unknown"\n'})}),"\n",(0,o.jsx)(n.h3,{id:"3-facial-expression-recognition",children:"3. Facial Expression Recognition"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from fer import FER\n\nclass EmotionRecognizer:\n    def __init__(self):\n        self.detector = FER(mtcnn=True)\n\n    def detect_emotion(self, image):\n        """Detect human emotion from face"""\n\n        emotions = self.detector.detect_emotions(image)\n\n        if not emotions:\n            return None\n\n        # Get dominant emotion\n        emotion_scores = emotions[0][\'emotions\']\n        dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n\n        return dominant_emotion, emotion_scores\n\n    def respond_to_emotion(self, emotion):\n        """Generate appropriate robot response"""\n\n        responses = {\n            \'happy\': "I\'m glad to see you\'re happy!",\n            \'sad\': "You seem sad. Is there anything I can help with?",\n            \'angry\': "I sense frustration. Let\'s take a moment.",\n            \'surprise\': "Something surprised you!",\n            \'neutral\': "How can I assist you today?"\n        }\n\n        return responses.get(emotion, "I\'m here to help.")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"4-gaze-and-attention",children:"4. Gaze and Attention"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class GazeController:\n    def __init__(self, robot):\n        self.robot = robot\n\n    def look_at_person(self, person_position):\n        """Orient robot head to look at person"""\n\n        # Compute head orientation\n        head_target = self.compute_look_at_orientation(\n            self.robot.head_position,\n            person_position\n        )\n\n        # Send head control command\n        self.robot.set_head_orientation(head_target)\n\n    def track_speaker(self, audio_direction):\n        """Track sound source"""\n\n        # Turn head toward sound\n        self.look_at_person(audio_direction)\n\n    def social_gaze(self, interaction_state):\n        """Implement natural gaze patterns"""\n\n        if interaction_state == \'listening\':\n            # Look at speaker\'s face\n            self.look_at_person(self.get_person_face())\n\n        elif interaction_state == \'speaking\':\n            # Alternate between looking at person and away\n            if random.random() < 0.7:  # 70% eye contact\n                self.look_at_person(self.get_person_face())\n            else:\n                self.look_away_naturally()\n\n        elif interaction_state == \'thinking\':\n            # Look up or to the side\n            self.look_at_thinking_direction()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"social-robot-behaviors",children:"Social Robot Behaviors"}),"\n",(0,o.jsx)(n.h3,{id:"proxemics-personal-space",children:"Proxemics (Personal Space)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ProxemicsManager:\n    def __init__(self):\n        # Personal space zones (in meters)\n        self.intimate_zone = 0.45\n        self.personal_zone = 1.2\n        self.social_zone = 3.6\n\n    def compute_appropriate_distance(self, context, relationship):\n        """Determine appropriate distance based on social context"""\n\n        if relationship == \'stranger\':\n            return self.social_zone\n        elif relationship == \'acquaintance\':\n            return self.personal_zone\n        elif relationship == \'close\':\n            return self.intimate_zone\n\n        # Default to personal zone\n        return self.personal_zone\n\n    def maintain_distance(self, person_position, desired_distance):\n        """Navigate to maintain appropriate social distance"""\n\n        current_distance = np.linalg.norm(\n            self.robot.position - person_position\n        )\n\n        distance_error = current_distance - desired_distance\n\n        if abs(distance_error) > 0.2:  # 20cm tolerance\n            # Move toward or away from person\n            direction = (person_position - self.robot.position)\n            direction = direction / np.linalg.norm(direction)\n\n            target_position = person_position - direction * desired_distance\n\n            self.robot.navigate_to(target_position)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"turn-taking-and-interruption-handling",children:"Turn-Taking and Interruption Handling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ConversationManager:\n    def __init__(self):\n        self.is_speaking = False\n        self.is_listening = False\n        self.interrupt_threshold = 0.5  # seconds\n\n    def manage_turn_taking(self):\n        """Implement natural turn-taking"""\n\n        # Detect when user starts speaking\n        if self.detect_user_speech():\n            if self.is_speaking:\n                # User interrupted robot\n                self.handle_interruption()\n            else:\n                # User\'s turn to speak\n                self.start_listening()\n\n        # Detect when user stops speaking\n        elif self.is_listening and self.detect_silence(duration=1.0):\n            # Robot\'s turn to respond\n            self.start_speaking()\n\n    def handle_interruption(self):\n        """Handle being interrupted gracefully"""\n\n        # Stop speaking immediately\n        self.stop_speech()\n\n        # Acknowledge interruption\n        responses = [\n            "Yes?",\n            "Go ahead.",\n            "What is it?"\n        ]\n        self.queue_response(random.choice(responses))\n\n        # Listen to user\n        self.start_listening()\n\n    def detect_end_of_turn(self, speech_audio):\n        """Detect if user has finished speaking"""\n\n        # Check for turn-yielding cues:\n        # - Silence duration\n        # - Falling intonation\n        # - Completion of syntactic phrase\n\n        silence_duration = self.get_silence_duration()\n        has_falling_intonation = self.detect_falling_pitch(speech_audio)\n\n        if silence_duration > 1.0 and has_falling_intonation:\n            return True\n\n        return False\n'})}),"\n",(0,o.jsx)(n.h3,{id:"expressiveness-and-body-language",children:"Expressiveness and Body Language"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ExpressiveBehavior:\n    def __init__(self, robot):\n        self.robot = robot\n\n    def express_emotion(self, emotion, intensity=0.5):\n        \"\"\"Express emotion through body language\"\"\"\n\n        if emotion == 'happy':\n            self.robot.set_head_tilt(15)  # Slight upward tilt\n            # Could also: wave, slight bounce motion\n\n        elif emotion == 'sad':\n            self.robot.set_head_tilt(-10)  # Downward tilt\n            self.robot.set_shoulder_slump(20)\n\n        elif emotion == 'confused':\n            self.robot.set_head_tilt(20, side='right')  # Head tilt to side\n            # Could also: scratch head gesture\n\n        elif emotion == 'confident':\n            self.robot.set_posture('upright')\n            self.robot.set_chest_out(10)\n\n    def animate_speech(self, text):\n        \"\"\"Add gestures during speech\"\"\"\n\n        # Simple rule-based gestures\n        if '?' in text:\n            self.robot.gesture('questioning_hands')\n\n        if any(word in text.lower() for word in ['yes', 'correct', 'right']):\n            self.robot.gesture('nod')\n\n        if any(word in text.lower() for word in ['no', 'wrong', 'incorrect']):\n            self.robot.gesture('shake_head')\n\n        # Pointing gestures for spatial references\n        if any(word in text.lower() for word in ['there', 'that', 'this']):\n            self.robot.gesture('point')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"safety-in-hri",children:"Safety in HRI"}),"\n",(0,o.jsx)(n.h3,{id:"collision-avoidance",children:"Collision Avoidance"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SafeHRI:\n    def __init__(self, robot):\n        self.robot = robot\n        self.safety_distance = 0.3  # meters\n\n    def safe_motion_planning(self, target_position):\n        """Plan motion that avoids colliding with humans"""\n\n        # Detect nearby humans\n        humans = self.detect_humans()\n\n        # Compute safe trajectory\n        trajectory = self.plan_trajectory(\n            start=self.robot.position,\n            goal=target_position,\n            obstacles=humans,\n            safety_margin=self.safety_distance\n        )\n\n        return trajectory\n\n    def emergency_stop(self):\n        """Immediately stop all motion"""\n\n        # Detect imminent collision\n        if self.detect_imminent_collision():\n            self.robot.stop_all_motion()\n            self.robot.lock_joints()\n            self.speak("Stopping for safety")\n\n    def compliant_contact(self):\n        """React safely to unexpected contact"""\n\n        if self.detect_contact():\n            # Immediately reduce force/torque\n            self.robot.set_compliant_mode()\n\n            # Move away from contact\n            contact_direction = self.get_contact_direction()\n            retreat_direction = -contact_direction\n\n            self.robot.move_slowly(retreat_direction)\n\n            self.speak("Sorry, I didn\'t mean to bump into you")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multimodal-interaction",children:"Multimodal Interaction"}),"\n",(0,o.jsx)(n.h3,{id:"combining-modalities",children:"Combining Modalities"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class MultimodalInteraction:\n    def __init__(self):\n        self.voice = VoiceInteraction()\n        self.gesture = GestureRecognizer()\n        self.gaze = GazeController()\n\n    def process_multimodal_input(self):\n        """Fuse information from multiple input channels"""\n\n        # Voice command\n        voice_input = self.voice.listen("audio.wav")\n\n        # Gesture\n        gesture = self.gesture.recognize_gesture(self.camera.get_frame())\n\n        # Combine information\n        if gesture == \'pointing\':\n            # User is pointing at something\n            point_direction = self.compute_point_direction(gesture)\n\n            if "that" in voice_input or "there" in voice_input:\n                # Resolve reference: "Bring me that" + pointing\n                object_location = point_direction\n                command = voice_input.replace("that", "object")\n\n                return {\n                    \'command\': command,\n                    \'target\': object_location,\n                    \'confidence\': 0.9\n                }\n\n        return {\n            \'command\': voice_input,\n            \'target\': None,\n            \'confidence\': 0.7\n        }\n\n    def multimodal_feedback(self, message):\n        """Provide feedback through multiple channels"""\n\n        # Voice\n        self.voice.speak(message)\n\n        # Gaze (look at user)\n        self.gaze.look_at_person(self.get_user_position())\n\n        # Gesture (nod)\n        self.robot.gesture(\'nod\')\n\n        # Could also: Display on screen, LED indicators, etc.\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-exercises",children:"Practical Exercises"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-1-conversational-robot",children:"Exercise 1: Conversational Robot"}),"\n",(0,o.jsx)(n.p,{children:"Implement a conversational interface:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Integrate speech recognition and TTS\n# 2. Add LLM for conversation\n# 3. Implement turn-taking\n# 4. Test with multiple users\n"})}),"\n",(0,o.jsx)(n.h3,{id:"exercise-2-gesture-control",children:"Exercise 2: Gesture Control"}),"\n",(0,o.jsx)(n.p,{children:"Create a gesture-based robot control system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Set up camera for hand tracking\n# 2. Implement gesture recognition\n# 3. Map gestures to robot actions\n# 4. Add visual feedback for recognized gestures\n"})}),"\n",(0,o.jsx)(n.h3,{id:"exercise-3-social-navigation",children:"Exercise 3: Social Navigation"}),"\n",(0,o.jsx)(n.p,{children:"Implement social-aware navigation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Detect people in environment\n# 2. Implement proxemics-aware path planning\n# 3. Respect personal space while navigating\n# 4. Test in crowded environment (simulation)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"HRI design requires understanding human psychology and social norms"}),"\n",(0,o.jsx)(n.li,{children:"Multiple interaction modalities enable natural communication"}),"\n",(0,o.jsx)(n.li,{children:"Safety is paramount in human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Expressiveness makes robots more understandable and relatable"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal interaction is more robust than single-channel"}),"\n",(0,o.jsx)(n.li,{children:"Social behaviors like gaze and proxemics improve user experience"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://link.springer.com/referencework/10.1007/978-3-319-32552-1",children:"Handbook of Human-Robot Interaction"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"http://robotics.usc.edu/interaction/",children:"Socially Assistive Robotics"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"http://wiki.ros.org/hri",children:"ROS HRI Stack"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["Continue to ",(0,o.jsx)(n.a,{href:"/docusaurus-robotics-book/fr/module4-vla/capstone-project",children:"Capstone Project"})," to integrate everything into a complete autonomous humanoid system."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);