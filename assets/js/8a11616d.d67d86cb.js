"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[8553],{2132:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module4-vla/llm-robotics","title":"LLM Integration in Robotics","description":"Overview","source":"@site/docs/module4-vla/llm-robotics.md","sourceDirName":"module4-vla","slug":"/module4-vla/llm-robotics","permalink":"/module4-vla/llm-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Pipeline","permalink":"/module4-vla/voice-to-action"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/module4-vla/cognitive-planning"}}');var a=i(4848),o=i(8453);const s={},l="LLM Integration in Robotics",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Why LLMs for Robotics?",id:"why-llms-for-robotics",level:2},{value:"Traditional Approach",id:"traditional-approach",level:3},{value:"LLM-Powered Approach",id:"llm-powered-approach",level:3},{value:"Architecture: From Language to Action",id:"architecture-from-language-to-action",level:2},{value:"The VLA Pipeline",id:"the-vla-pipeline",level:3},{value:"1. Speech-to-Text (OpenAI Whisper)",id:"1-speech-to-text-openai-whisper",level:4},{value:"2. LLM Task Planning (GPT-4)",id:"2-llm-task-planning-gpt-4",level:4},{value:"3. Action Primitive Execution",id:"3-action-primitive-execution",level:4},{value:"Grounding: Connecting Words to Physical World",id:"grounding-connecting-words-to-physical-world",level:2},{value:"The Grounding Problem",id:"the-grounding-problem",level:3},{value:"Semantic Mapping",id:"semantic-mapping",level:3},{value:"Context and Memory",id:"context-and-memory",level:2},{value:"Maintaining Conversation Context",id:"maintaining-conversation-context",level:3},{value:"Example Contextual Dialogue",id:"example-contextual-dialogue",level:3},{value:"Safety and Constraints",id:"safety-and-constraints",level:2},{value:"Constraining LLM Outputs",id:"constraining-llm-outputs",level:3},{value:"Practical Implementation",id:"practical-implementation",level:2},{value:"Full VLA System",id:"full-vla-system",level:3},{value:"Challenges and Limitations",id:"challenges-and-limitations",level:2},{value:"Current Challenges",id:"current-challenges",level:3},{value:"Mitigation Strategies",id:"mitigation-strategies",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic Voice Commands",id:"exercise-1-basic-voice-commands",level:3},{value:"Exercise 2: Contextual Understanding",id:"exercise-2-contextual-understanding",level:3},{value:"Exercise 3: Safe Action Planning",id:"exercise-3-safe-action-planning",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Resources",id:"resources",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"llm-integration-in-robotics",children:"LLM Integration in Robotics"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"The convergence of Large Language Models (LLMs) and robotics represents a paradigm shift in how robots understand and execute tasks. This module explores how to integrate conversational AI into robotic systems."}),"\n",(0,a.jsx)(n.h2,{id:"why-llms-for-robotics",children:"Why LLMs for Robotics?"}),"\n",(0,a.jsx)(n.h3,{id:"traditional-approach",children:"Traditional Approach"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Hardcoded commands: ",(0,a.jsx)(n.code,{children:"move_forward(2.5)"}),", ",(0,a.jsx)(n.code,{children:'grasp_object("cup")'})]}),"\n",(0,a.jsx)(n.li,{children:"Limited flexibility and natural interaction"}),"\n",(0,a.jsx)(n.li,{children:"Requires programming for every new task"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"llm-powered-approach",children:"LLM-Powered Approach"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'Natural language commands: "Please bring me the red cup from the kitchen table"'}),"\n",(0,a.jsx)(n.li,{children:"Contextual understanding and reasoning"}),"\n",(0,a.jsx)(n.li,{children:"Adaptive to new scenarios without reprogramming"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"architecture-from-language-to-action",children:"Architecture: From Language to Action"}),"\n",(0,a.jsx)(n.h3,{id:"the-vla-pipeline",children:"The VLA Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"User Speech Input \u2192 Speech-to-Text \u2192 LLM Planning \u2192 Action Primitives \u2192 ROS 2 Execution\n"})}),"\n",(0,a.jsx)(n.h4,{id:"1-speech-to-text-openai-whisper",children:"1. Speech-to-Text (OpenAI Whisper)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model\nmodel = whisper.load_model("base")\n\n# Transcribe audio\nresult = model.transcribe("audio.mp3")\nprint(result["text"])\n# Output: "Pick up the blue box and place it on the shelf"\n'})}),"\n",(0,a.jsx)(n.h4,{id:"2-llm-task-planning-gpt-4",children:"2. LLM Task Planning (GPT-4)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\n\ndef natural_language_to_plan(command: str) -> list:\n    """Convert natural language to robot action plan"""\n\n    system_prompt = """You are a robot task planner. Given a natural language\n    command, break it down into a sequence of robot primitives:\n    - navigate_to(location)\n    - detect_object(description)\n    - grasp_object(object_id)\n    - place_object(location)\n    - speak(text)\n\n    Return a JSON list of actions."""\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": system_prompt},\n            {"role": "user", "content": command}\n        ]\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,a.jsx)(n.p,{children:"Example output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'[\n    {"action": "navigate_to", "params": {"location": "kitchen"}},\n    {"action": "detect_object", "params": {"description": "blue box"}},\n    {"action": "grasp_object", "params": {"object_id": "detected_object_1"}},\n    {"action": "navigate_to", "params": {"location": "shelf"}},\n    {"action": "place_object", "params": {"location": "shelf_surface"}},\n    {"action": "speak", "params": {"text": "Task completed"}}\n]\n'})}),"\n",(0,a.jsx)(n.h4,{id:"3-action-primitive-execution",children:"3. Action Primitive Execution"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\n\nclass ActionExecutor(Node):\n    def __init__(self):\n        super().__init__(\'action_executor\')\n        self.nav_pub = self.create_publisher(PoseStamped, \'/goal_pose\', 10)\n        self.grasp_pub = self.create_publisher(String, \'/grasp_command\', 10)\n\n    def execute_plan(self, plan: list):\n        for action in plan:\n            action_type = action["action"]\n            params = action["params"]\n\n            if action_type == "navigate_to":\n                self.navigate(params["location"])\n            elif action_type == "grasp_object":\n                self.grasp(params["object_id"])\n            elif action_type == "place_object":\n                self.place(params["location"])\n\n    def navigate(self, location: str):\n        # Publish navigation goal\n        goal = PoseStamped()\n        # ... set goal based on location\n        self.nav_pub.publish(goal)\n\n    def grasp(self, object_id: str):\n        # Send grasp command\n        msg = String()\n        msg.data = f"grasp {object_id}"\n        self.grasp_pub.publish(msg)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"grounding-connecting-words-to-physical-world",children:"Grounding: Connecting Words to Physical World"}),"\n",(0,a.jsx)(n.h3,{id:"the-grounding-problem",children:"The Grounding Problem"}),"\n",(0,a.jsx)(n.p,{children:"LLMs understand language but not physical space. We need to ground abstract concepts:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'"Kitchen" \u2192 GPS coordinates or map location'}),"\n",(0,a.jsx)(n.li,{children:'"Blue box" \u2192 Object detected by vision system'}),"\n",(0,a.jsx)(n.li,{children:'"Gently" \u2192 Force control parameters'}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"semantic-mapping",children:"Semantic Mapping"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class SemanticMap:\n    def __init__(self):\n        self.locations = {\n            "kitchen": {"x": 5.0, "y": 3.0, "z": 0.0},\n            "living room": {"x": -2.0, "y": 1.0, "z": 0.0},\n            "shelf": {"x": 3.0, "y": -1.0, "z": 0.8}\n        }\n\n    def ground_location(self, location_name: str):\n        """Convert location name to coordinates"""\n        return self.locations.get(location_name.lower())\n\n    def ground_object(self, description: str, vision_detections: list):\n        """Match description to detected objects"""\n        # Use CLIP or similar vision-language model\n        for obj in vision_detections:\n            similarity = self.compute_similarity(description, obj.features)\n            if similarity > threshold:\n                return obj\n        return None\n'})}),"\n",(0,a.jsx)(n.h2,{id:"context-and-memory",children:"Context and Memory"}),"\n",(0,a.jsx)(n.h3,{id:"maintaining-conversation-context",children:"Maintaining Conversation Context"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ConversationalRobot:\n    def __init__(self):\n        self.conversation_history = []\n        self.scene_context = {}\n\n    def process_command(self, user_input: str):\n        # Add to conversation history\n        self.conversation_history.append({\n            "role": "user",\n            "content": user_input\n        })\n\n        # Include scene context\n        context_prompt = f"""\n        Current location: {self.scene_context[\'location\']}\n        Visible objects: {self.scene_context[\'objects\']}\n        Previous actions: {self.conversation_history[-3:]}\n        """\n\n        # Get LLM response with full context\n        response = self.llm_plan(context_prompt + user_input)\n\n        return response\n'})}),"\n",(0,a.jsx)(n.h3,{id:"example-contextual-dialogue",children:"Example Contextual Dialogue"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'User: "Is there a cup on the table?"\nRobot: [Checks vision] "Yes, I see a red cup on the kitchen table."\nUser: "Bring it to me."\nRobot: [Understands "it" refers to the cup] "Navigating to kitchen table..."\n'})}),"\n",(0,a.jsx)(n.h2,{id:"safety-and-constraints",children:"Safety and Constraints"}),"\n",(0,a.jsx)(n.h3,{id:"constraining-llm-outputs",children:"Constraining LLM Outputs"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def validate_action_plan(plan: list) -> bool:\n    """Ensure plan is safe and feasible"""\n\n    for action in plan:\n        # Check action is in allowed set\n        if action["action"] not in ALLOWED_ACTIONS:\n            return False\n\n        # Check spatial constraints\n        if action["action"] == "navigate_to":\n            location = action["params"]["location"]\n            if not is_reachable(location):\n                return False\n\n        # Check object manipulation constraints\n        if action["action"] == "grasp_object":\n            if not is_safe_to_grasp(action["params"]["object_id"]):\n                return False\n\n    return True\n'})}),"\n",(0,a.jsx)(n.h2,{id:"practical-implementation",children:"Practical Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"full-vla-system",children:"Full VLA System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class VLARobot(Node):\n    def __init__(self):\n        super().__init__(\'vla_robot\')\n        self.whisper_model = whisper.load_model("base")\n        self.llm_client = openai.Client()\n        self.action_executor = ActionExecutor()\n        self.semantic_map = SemanticMap()\n\n    def voice_command_callback(self, audio_path: str):\n        # 1. Speech-to-Text\n        text = self.whisper_model.transcribe(audio_path)["text"]\n        self.get_logger().info(f"Heard: {text}")\n\n        # 2. LLM Planning\n        plan = self.llm_plan(text)\n        self.get_logger().info(f"Plan: {plan}")\n\n        # 3. Ground plan to physical world\n        grounded_plan = self.ground_plan(plan)\n\n        # 4. Validate safety\n        if not validate_action_plan(grounded_plan):\n            self.speak("I cannot safely execute this command")\n            return\n\n        # 5. Execute\n        self.action_executor.execute_plan(grounded_plan)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"challenges-and-limitations",children:"Challenges and Limitations"}),"\n",(0,a.jsx)(n.h3,{id:"current-challenges",children:"Current Challenges"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Hallucination"}),": LLMs may plan infeasible actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": API calls can introduce delays"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cost"}),": Frequent LLM queries can be expensive"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Grounding"}),": Connecting abstract concepts to physical world"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety"}),": Ensuring safe execution of generated plans"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"mitigation-strategies",children:"Mitigation Strategies"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation layers"}),": Check plan feasibility before execution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Local models"}),": Use smaller, local LLMs for latency-critical applications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Caching"}),": Store common command patterns"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human-in-the-loop"}),": Request confirmation for uncertain actions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-1-basic-voice-commands",children:"Exercise 1: Basic Voice Commands"}),"\n",(0,a.jsx)(n.p,{children:"Implement a simple voice command system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# TODO: Student implementation\n# 1. Set up Whisper for speech recognition\n# 2. Create LLM prompt for task planning\n# 3. Implement basic action primitives (navigate, speak)\n# 4. Test with simple commands like "Go to the kitchen"\n'})}),"\n",(0,a.jsx)(n.h3,{id:"exercise-2-contextual-understanding",children:"Exercise 2: Contextual Understanding"}),"\n",(0,a.jsx)(n.p,{children:"Extend the system to handle contextual references:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# TODO: Student implementation\n# 1. Maintain conversation history\n# 2. Handle pronouns ("it", "there", "that")\n# 3. Track mentioned objects and locations\n# 4. Test multi-turn conversations\n'})}),"\n",(0,a.jsx)(n.h3,{id:"exercise-3-safe-action-planning",children:"Exercise 3: Safe Action Planning"}),"\n",(0,a.jsx)(n.p,{children:"Add safety constraints:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# TODO: Student implementation\n# 1. Define workspace boundaries\n# 2. Identify forbidden actions\n# 3. Implement action validation\n# 4. Test rejection of unsafe commands\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LLMs enable natural language interaction with robots"}),"\n",(0,a.jsx)(n.li,{children:"The VLA pipeline: Speech \u2192 Planning \u2192 Grounding \u2192 Execution"}),"\n",(0,a.jsx)(n.li,{children:"Grounding connects abstract language to physical reality"}),"\n",(0,a.jsx)(n.li,{children:"Safety validation is critical for autonomous execution"}),"\n",(0,a.jsx)(n.li,{children:"Context and memory enable more natural conversations"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://python.langchain.com/docs/use_cases/robotics",children:"LangChain for Robotics"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"Vision-Language-Action Models Paper"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.p,{children:["Continue to ",(0,a.jsx)(n.a,{href:"/module4-vla/cognitive-planning",children:"Cognitive Planning"})," to learn about advanced reasoning for complex tasks."]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var t=i(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);