"use strict";(globalThis.webpackChunkrobotics_book=globalThis.webpackChunkrobotics_book||[]).push([[1670],{8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var s=r(6540);const t={},o=s.createContext(t);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),s.createElement(o.Provider,{value:n},e.children)}},9100:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>g,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module1-ros2/python-ros2","title":"Python and ROS 2 Integration","description":"Bridging Python AI agents to ROS 2 controllers using rclpy","source":"@site/docs/module1-ros2/python-ros2.md","sourceDirName":"module1-ros2","slug":"/module1-ros2/python-ros2","permalink":"/docusaurus-robotics-book/module1-ros2/python-ros2","draft":false,"unlisted":false,"tags":[],"version":"current","lastUpdatedAt":1764862720000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Python and ROS 2 Integration","description":"Bridging Python AI agents to ROS 2 controllers using rclpy","keywords":["ROS 2","Python","rclpy","AI agents","integration"]},"sidebar":"tutorialSidebar","previous":{"title":"Nodes, Topics, and Services","permalink":"/docusaurus-robotics-book/module1-ros2/nodes-topics-services"},"next":{"title":"URDF for Humanoids","permalink":"/docusaurus-robotics-book/module1-ros2/urdf-humanoids"}}');var t=r(4848),o=r(8453);const i={sidebar_position:3,title:"Python and ROS 2 Integration",description:"Bridging Python AI agents to ROS 2 controllers using rclpy",keywords:["ROS 2","Python","rclpy","AI agents","integration"]},l="Python and ROS 2 Integration",a={},c=[{value:"Bridging AI to Robotics",id:"bridging-ai-to-robotics",level:2},{value:"Why Python for ROS 2?",id:"why-python-for-ros-2",level:2},{value:"Advantages",id:"advantages",level:3},{value:"Limitations",id:"limitations",level:3},{value:"When to Use Each",id:"when-to-use-each",level:3},{value:"rclpy Fundamentals",id:"rclpy-fundamentals",level:2},{value:"Creating a Node",id:"creating-a-node",level:3},{value:"Node Lifecycle",id:"node-lifecycle",level:3},{value:"Timers and Periodic Execution",id:"timers-and-periodic-execution",level:2},{value:"Creating Timers",id:"creating-timers",level:3},{value:"Multiple Timers",id:"multiple-timers",level:3},{value:"Real-World Example: AI Object Detector Node",id:"real-world-example-ai-object-detector-node",level:2},{value:"Running the Detector",id:"running-the-detector",level:3},{value:"Integrating LLMs for Cognitive Control",id:"integrating-llms-for-cognitive-control",level:2},{value:"OpenAI GPT Integration Example",id:"openai-gpt-integration-example",level:3},{value:"Asynchronous Operations",id:"asynchronous-operations",level:2},{value:"Using async/await with ROS 2",id:"using-asyncawait-with-ros-2",level:3},{value:"Multi-Threaded Nodes",id:"multi-threaded-nodes",level:2},{value:"Parallel Processing Example",id:"parallel-processing-example",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Parameter Management",id:"1-parameter-management",level:3},{value:"2. Graceful Shutdown",id:"2-graceful-shutdown",level:3},{value:"3. Logging Best Practices",id:"3-logging-best-practices",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Lab Exercise",id:"lab-exercise",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"python-and-ros-2-integration",children:"Python and ROS 2 Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"bridging-ai-to-robotics",children:"Bridging AI to Robotics"}),"\n",(0,t.jsxs)(n.p,{children:["Modern AI systems are predominantly written in Python (TensorFlow, PyTorch, OpenAI APIs), while robotics requires real-time control. ",(0,t.jsx)(n.strong,{children:"rclpy"})," (ROS Client Library for Python) bridges this gap, allowing you to:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Run AI models (object detection, LLMs) as ROS 2 nodes"}),"\n",(0,t.jsx)(n.li,{children:"Subscribe to sensor data (camera, LIDAR) for AI processing"}),"\n",(0,t.jsx)(n.li,{children:"Publish decisions/actions to robot controllers"}),"\n",(0,t.jsx)(n.li,{children:"Integrate pre-trained models into the robot control loop"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"why-python-for-ros-2",children:"Why Python for ROS 2?"}),"\n",(0,t.jsx)(n.h3,{id:"advantages",children:"Advantages"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AI/ML ecosystem"}),": NumPy, OpenCV, TensorFlow, PyTorch"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rapid prototyping"}),": Faster development than C++"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Easy integration"}),": Call APIs (OpenAI, cloud services) directly"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rich libraries"}),": Extensive robotics packages (scipy, scikit-learn)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"limitations",children:"Limitations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),": Slower than C++ for compute-intensive tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GIL (Global Interpreter Lock)"}),": Limits true multi-threading"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory"}),": Higher memory usage than C++"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"when-to-use-each",children:"When to Use Each"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Use Case"}),(0,t.jsx)(n.th,{children:"Language"}),(0,t.jsx)(n.th,{children:"Reason"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"AI inference (object detection, LLMs)"}),(0,t.jsx)(n.td,{children:"Python"}),(0,t.jsx)(n.td,{children:"AI libraries are Python-native"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"High-level planning & decision making"}),(0,t.jsx)(n.td,{children:"Python"}),(0,t.jsx)(n.td,{children:"Rapid prototyping, flexibility"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Real-time control loops (50-1000 Hz)"}),(0,t.jsx)(n.td,{children:"C++"}),(0,t.jsx)(n.td,{children:"Low latency, deterministic timing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Sensor drivers (camera, LIDAR)"}),(0,t.jsx)(n.td,{children:"C++"}),(0,t.jsx)(n.td,{children:"Performance, vendor SDKs often C++"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Visualization & tools"}),(0,t.jsx)(n.td,{children:"Python"}),(0,t.jsx)(n.td,{children:"Rapid development"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Best practice"}),": Use Python for AI/planning, C++ for control loops, communicate via ROS 2 topics."]}),"\n",(0,t.jsx)(n.h2,{id:"rclpy-fundamentals",children:"rclpy Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"creating-a-node",children:"Creating a Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\n\nclass MyNode(Node):\n    def __init__(self):\n        super().__init__('my_node_name')\n        self.get_logger().info('Node started')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MyNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"node-lifecycle",children:"Node Lifecycle"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# 1. Initialize ROS 2\nrclpy.init(args=args)\n\n# 2. Create node instance\nnode = MyNode()\n\n# 3. Spin (process callbacks)\nrclpy.spin(node)  # Blocks until shutdown\n\n# 4. Cleanup\nnode.destroy_node()\nrclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"timers-and-periodic-execution",children:"Timers and Periodic Execution"}),"\n",(0,t.jsx)(n.h3,{id:"creating-timers",children:"Creating Timers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PeriodicNode(Node):\n    def __init__(self):\n        super().__init__('periodic_node')\n\n        # Execute callback every 0.1 seconds (10 Hz)\n        self.timer = self.create_timer(0.1, self.timer_callback)\n        self.count = 0\n\n    def timer_callback(self):\n        self.get_logger().info(f'Timer callback {self.count}')\n        self.count += 1\n"})}),"\n",(0,t.jsx)(n.h3,{id:"multiple-timers",children:"Multiple Timers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class MultiTimerNode(Node):\n    def __init__(self):\n        super().__init__('multi_timer_node')\n\n        # Fast loop: Sensor reading at 50 Hz\n        self.sensor_timer = self.create_timer(0.02, self.read_sensors)\n\n        # Slow loop: Logging at 1 Hz\n        self.log_timer = self.create_timer(1.0, self.log_status)\n\n    def read_sensors(self):\n        # High-frequency sensor processing\n        pass\n\n    def log_status(self):\n        # Low-frequency status logging\n        pass\n"})}),"\n",(0,t.jsx)(n.h2,{id:"real-world-example-ai-object-detector-node",children:"Real-World Example: AI Object Detector Node"}),"\n",(0,t.jsx)(n.p,{children:"Integrating a pre-trained YOLO model with ROS 2:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\n# YOLOv8 (using ultralytics package)\nfrom ultralytics import YOLO\n\nclass YOLODetectorNode(Node):\n    def __init__(self):\n        super().__init__('yolo_detector')\n\n        # Declare parameters\n        self.declare_parameter('model_path', 'yolov8n.pt')\n        self.declare_parameter('confidence_threshold', 0.5)\n\n        # Load YOLO model\n        model_path = self.get_parameter('model_path').value\n        self.model = YOLO(model_path)\n        self.conf_threshold = self.get_parameter('confidence_threshold').value\n\n        # ROS 2 interfaces\n        self.bridge = CvBridge()\n\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/detections',\n            10\n        )\n\n        self.get_logger().info(f'YOLO Detector started with model: {model_path}')\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Run YOLO inference\n        results = self.model(cv_image, conf=self.conf_threshold, verbose=False)\n\n        # Convert to ROS 2 Detection2DArray message\n        detection_msg = Detection2DArray()\n        detection_msg.header = msg.header\n\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                # Extract detection info\n                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                confidence = float(box.conf[0])\n                class_id = int(box.cls[0])\n                class_name = result.names[class_id]\n\n                # Create Detection2D message\n                detection = Detection2D()\n\n                # Bounding box center and size\n                detection.bbox.center.position.x = float((x1 + x2) / 2)\n                detection.bbox.center.position.y = float((y1 + y2) / 2)\n                detection.bbox.size_x = float(x2 - x1)\n                detection.bbox.size_y = float(y2 - y1)\n\n                # Object hypothesis\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.hypothesis.class_id = class_name\n                hypothesis.hypothesis.score = confidence\n                detection.results.append(hypothesis)\n\n                detection_msg.detections.append(detection)\n\n        # Publish detections\n        self.detection_pub.publish(detection_msg)\n\n        self.get_logger().info(\n            f'Detected {len(detection_msg.detections)} objects',\n            throttle_duration_sec=1.0  # Log at most once per second\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = YOLODetectorNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"running-the-detector",children:"Running the Detector"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run camera node (or use rosbag)\nros2 run usb_cam usb_cam_node_exe\n\n# Terminal 2: Run YOLO detector with custom parameters\nros2 run my_package yolo_detector --ros-args \\\n  -p model_path:=yolov8m.pt \\\n  -p confidence_threshold:=0.6\n\n# Terminal 3: Visualize detections\nros2 run rqt_image_view rqt_image_view /detections/visualization\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integrating-llms-for-cognitive-control",children:"Integrating LLMs for Cognitive Control"}),"\n",(0,t.jsx)(n.h3,{id:"openai-gpt-integration-example",children:"OpenAI GPT Integration Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport openai\nimport os\n\nclass LLMPlannerNode(Node):\n    def __init__(self):\n        super().__init__('llm_planner')\n\n        # OpenAI API key (use environment variable)\n        openai.api_key = os.getenv('OPENAI_API_KEY')\n\n        # Subscribe to voice commands (from speech-to-text)\n        self.voice_sub = self.create_subscription(\n            String,\n            '/voice_command',\n            self.voice_callback,\n            10\n        )\n\n        # Publish robot velocity commands\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # System prompt for robot control\n        self.system_prompt = \"\"\"\n        You are a robot controller. Convert natural language commands into\n        simple motion primitives: forward, backward, turn_left, turn_right, stop.\n        Respond ONLY with the command name, no explanation.\n        \"\"\"\n\n        self.get_logger().info('LLM Planner Node started')\n\n    def voice_callback(self, msg):\n        command_text = msg.data\n        self.get_logger().info(f'Received command: {command_text}')\n\n        # Query LLM for action\n        action = self.query_llm(command_text)\n\n        # Execute action\n        self.execute_action(action)\n\n    def query_llm(self, user_input):\n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": user_input}\n                ],\n                temperature=0.0,  # Deterministic\n                max_tokens=10\n            )\n\n            action = response.choices[0].message.content.strip().lower()\n            self.get_logger().info(f'LLM action: {action}')\n            return action\n\n        except Exception as e:\n            self.get_logger().error(f'LLM query failed: {e}')\n            return 'stop'\n\n    def execute_action(self, action):\n        twist = Twist()\n\n        if action == 'forward':\n            twist.linear.x = 0.5\n        elif action == 'backward':\n            twist.linear.x = -0.5\n        elif action == 'turn_left':\n            twist.angular.z = 0.5\n        elif action == 'turn_right':\n            twist.angular.z = -0.5\n        elif action == 'stop':\n            twist.linear.x = 0.0\n            twist.angular.z = 0.0\n        else:\n            self.get_logger().warn(f'Unknown action: {action}')\n            return\n\n        self.cmd_pub.publish(twist)\n        self.get_logger().info(f'Executing: {action}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlannerNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"asynchronous-operations",children:"Asynchronous Operations"}),"\n",(0,t.jsx)(n.h3,{id:"using-asyncawait-with-ros-2",children:"Using async/await with ROS 2"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom rclpy.executors import MultiThreadedExecutor\nfrom std_srvs.srv import Trigger\nimport asyncio\n\nclass AsyncNode(Node):\n    def __init__(self):\n        super().__init__('async_node')\n\n        # Service server\n        self.srv = self.create_service(\n            Trigger,\n            'long_task',\n            self.long_task_callback\n        )\n\n    async def long_task_callback(self, request, response):\n        self.get_logger().info('Starting long task...')\n\n        # Simulate long-running operation\n        await asyncio.sleep(5.0)\n\n        self.get_logger().info('Long task completed')\n        response.success = True\n        response.message = 'Task completed successfully'\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AsyncNode()\n\n    # Use MultiThreadedExecutor for async operations\n    executor = MultiThreadedExecutor()\n    executor.add_node(node)\n\n    try:\n        executor.spin()\n    finally:\n        executor.shutdown()\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"multi-threaded-nodes",children:"Multi-Threaded Nodes"}),"\n",(0,t.jsx)(n.h3,{id:"parallel-processing-example",children:"Parallel Processing Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.callback_groups import MutuallyExclusiveCallbackGroup\nfrom sensor_msgs.msg import Image\nimport threading\n\nclass ParallelProcessorNode(Node):\n    def __init__(self):\n        super().__init__('parallel_processor')\n\n        # Create separate callback groups for parallel execution\n        self.camera1_group = MutuallyExclusiveCallbackGroup()\n        self.camera2_group = MutuallyExclusiveCallbackGroup()\n\n        # Subscribe to two cameras with separate groups\n        self.camera1_sub = self.create_subscription(\n            Image,\n            '/camera1/image',\n            self.camera1_callback,\n            10,\n            callback_group=self.camera1_group\n        )\n\n        self.camera2_sub = self.create_subscription(\n            Image,\n            '/camera2/image',\n            self.camera2_callback,\n            10,\n            callback_group=self.camera2_group\n        )\n\n    def camera1_callback(self, msg):\n        # Process camera 1 (can run in parallel with camera2_callback)\n        thread_id = threading.get_ident()\n        self.get_logger().info(f'Camera 1 processing in thread {thread_id}')\n        # Heavy processing here...\n\n    def camera2_callback(self, msg):\n        # Process camera 2 (can run in parallel with camera1_callback)\n        thread_id = threading.get_ident()\n        self.get_logger().info(f'Camera 2 processing in thread {thread_id}')\n        # Heavy processing here...\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ParallelProcessorNode()\n\n    # Use MultiThreadedExecutor with 4 threads\n    executor = MultiThreadedExecutor(num_threads=4)\n    executor.add_node(node)\n\n    try:\n        executor.spin()\n    finally:\n        executor.shutdown()\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-parameter-management",children:"1. Parameter Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ConfigurableNode(Node):\n    def __init__(self):\n        super().__init__('configurable_node')\n\n        # Declare parameters with defaults\n        self.declare_parameter('update_rate', 10.0)\n        self.declare_parameter('max_speed', 1.0)\n        self.declare_parameter('robot_name', 'default_robot')\n\n        # Get parameter values\n        self.update_rate = self.get_parameter('update_rate').value\n        self.max_speed = self.get_parameter('max_speed').value\n        self.robot_name = self.get_parameter('robot_name').value\n\n        # Register parameter change callback\n        self.add_on_set_parameters_callback(self.parameter_callback)\n\n    def parameter_callback(self, params):\n        for param in params:\n            if param.name == 'update_rate' and param.value > 0:\n                self.update_rate = param.value\n                # Recreate timer with new rate\n                self.get_logger().info(f'Update rate changed to {param.value}')\n\n        return rclpy.parameter.SetParametersResult(successful=True)\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-graceful-shutdown",children:"2. Graceful Shutdown"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import signal\nimport sys\n\nclass GracefulNode(Node):\n    def __init__(self):\n        super().__init__('graceful_node')\n        self.running = True\n\n        # Register signal handlers\n        signal.signal(signal.SIGINT, self.signal_handler)\n        signal.signal(signal.SIGTERM, self.signal_handler)\n\n    def signal_handler(self, sig, frame):\n        self.get_logger().info('Shutting down gracefully...')\n        self.running = False\n        # Cleanup operations\n        self.cleanup()\n        sys.exit(0)\n\n    def cleanup(self):\n        # Stop motors, close connections, save state, etc.\n        self.get_logger().info('Cleanup completed')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = GracefulNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.cleanup()\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"3-logging-best-practices",children:"3. Logging Best Practices"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class LoggingNode(Node):\n    def __init__(self):\n        super().__init__('logging_node')\n\n        # Different log levels\n        self.get_logger().debug('Debug message')      # Verbose info\n        self.get_logger().info('Info message')         # Normal operation\n        self.get_logger().warn('Warning message')      # Potential issues\n        self.get_logger().error('Error message')       # Errors occurred\n        self.get_logger().fatal('Fatal message')       # Critical failures\n\n        # Throttled logging (max once per second)\n        self.timer = self.create_timer(0.01, self.fast_callback)\n        self.count = 0\n\n    def fast_callback(self):\n        self.count += 1\n        # Only log once per second, even though callback runs 100 Hz\n        self.get_logger().info(\n            f'Count: {self.count}',\n            throttle_duration_sec=1.0\n        )\n"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"You've mastered Python and ROS 2 integration. Next, learn how to define robot morphology using URDF for humanoids."}),"\n",(0,t.jsxs)(n.p,{children:["\ud83d\udc49 ",(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"urdf-humanoids",children:"Next: URDF for Humanoids \u2192"})})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.admonition,{title:"Performance Tip",type:"tip",children:[(0,t.jsx)(n.p,{children:"For AI inference in ROS 2:"}),(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use GPU acceleration"}),": CUDA/TensorRT for neural networks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch processing"}),": Accumulate messages, process in batches"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Async operations"}),": Don't block callbacks with slow operations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Quality of Service"}),": Use BEST_EFFORT for sensor data to reduce latency"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-threading"}),": Separate callback groups for parallel processing"]}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"lab-exercise",children:"Lab Exercise"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Build an AI-powered robot controller"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object detector node"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Subscribe to ",(0,t.jsx)(n.code,{children:"/camera/image"})]}),"\n",(0,t.jsx)(n.li,{children:"Run YOLOv8 detection"}),"\n",(0,t.jsxs)(n.li,{children:["Publish to ",(0,t.jsx)(n.code,{children:"/detections"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Decision node"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Subscribe to ",(0,t.jsx)(n.code,{children:"/detections"})]}),"\n",(0,t.jsxs)(n.li,{children:['If "person" detected, send ',(0,t.jsx)(n.code,{children:"/cmd_vel"})," to approach"]}),"\n",(0,t.jsx)(n.li,{children:'If "stop_sign" detected, send stop command'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Test"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Run with webcam or rosbag"}),"\n",(0,t.jsx)(n.li,{children:"Verify detection and control logic"}),"\n",(0,t.jsx)(n.li,{children:"Measure latency (detection to command)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bonus"}),": Add LLM integration for natural language commands."]})]})}function g(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);